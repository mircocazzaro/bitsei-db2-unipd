{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the Loa Angeles Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "# CHECK DATE \n",
    "from datetime import datetime, date, timedelta\n",
    "import urllib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "activeBusinessesData = 'datasets/ACTIVE BUSINESSES/fixed-Listing_of_Active_Businesses_parsed.csv'\n",
    "closedBusinessesData = 'datasets/CLOSED BUSINESSES/parsed-All_Closed_Businesses_20231101_PARSED.csv'\n",
    "laCovidData = 'datasets/COVID DATA/sorted_los_angeles_covid_data.csv'\n",
    "crimeData1 = 'datasets/CRIME DATA/parsed-Crime_Data_from_2020_to_Present-part1.csv'\n",
    "crimeData2 = 'datasets/CRIME DATA/parsed-Crime_Data_from_2020_to_Present-part2.csv'\n",
    "crimeData3 = 'datasets/CRIME DATA/parsed-Crime_Data_from_2020_to_Present-part3.csv'\n",
    "crimeCodesDescData = 'datasets/CRIME DATA/CrimesCodesAndDesc_listed.csv'\n",
    "moCodesData = 'datasets/CRIME DATA/CrimesCodesAndDesc_listed.csv'\n",
    "naicsData = 'datasets/CLOSED BUSINESSES/2022_NAICS_Descriptions.csv'\n",
    "weaponData = 'datasets/CRIME DATA/weapon_ds.csv'\n",
    "\n",
    "\n",
    "# saving folder\n",
    "savePath =  path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "#CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "LAO = Namespace(\"http://www.bitsei.it/losAngelesOntology/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Last_Update</th>\n",
       "      <th>Active</th>\n",
       "      <th>Deaths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solodata</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26</th>\n",
       "      <td>2023-12-26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <td>2023-12-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2193 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Last_Update  Active  Deaths\n",
       "solodata                              \n",
       "2018-01-01  2018-01-01       0       0\n",
       "2018-01-02  2018-01-02       0       0\n",
       "2018-01-03  2018-01-03       0       0\n",
       "2018-01-04  2018-01-04       0       0\n",
       "2018-01-05  2018-01-05       0       0\n",
       "...                ...     ...     ...\n",
       "2023-12-26  2023-12-26       0       0\n",
       "2023-12-27  2023-12-27       0       0\n",
       "2023-12-28  2023-12-28       0       0\n",
       "2023-12-29  2023-12-29       0       0\n",
       "2023-12-30  2023-12-30       0       0\n",
       "\n",
       "[2193 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "laCovid = pd.read_csv(laCovidData, sep=',') #, dtype={'Active':int, 'Deaths':int}\n",
    "\n",
    "laCovid[\"Last_Update\"] = pd.to_datetime(laCovid['Last_Update'])\n",
    "laCovid['solodata'] = laCovid['Last_Update'].dt.date\n",
    "laCovid['Active'] = laCovid['Active'].astype('Int64')\n",
    "laCovid['Deaths'] = laCovid['Deaths'].astype('Int64')\n",
    "\n",
    "laCovid = laCovid[['Last_Update', 'solodata', 'Active', 'Deaths']]\n",
    "\n",
    "start_date = pd.to_datetime('2018-01-01')\n",
    "end_date = pd.to_datetime('2023-12-31')\n",
    "for i in daterange(start_date, end_date):\n",
    "    if i.date() not in laCovid['solodata'].values:\n",
    "        new_row = {\n",
    "            'solodata': i.date(),\n",
    "            'Last_Update' : i, #pd.to_datetime(str(i), '%Y-%m-%d %H%M%S')\n",
    "            'Active': 0,\n",
    "            'Deaths': 0\n",
    "        }\n",
    "        laCovid = pd.concat([laCovid, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "laCovid.sort_values(by='solodata', inplace=True)\n",
    "laCovid.set_index(\"solodata\", inplace=True)\n",
    "laCovid.to_csv(\"coviddc.csv\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)\n",
    "\n",
    "laCovid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:11: UserWarning: Code: datetime is not defined in namespace XSD\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 687 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the league dataframe\n",
    "for index, row in laCovid.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = \"day\" + str(index)\n",
    "    Day = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Day, RDF.type, LAO.Day))\n",
    "    g.add((Day, LAO['hasDate'], Literal(str(row['Last_Update']), datatype=XSD.datetime)))    \n",
    "    g.add((Day, LAO['hasActiveCases'], Literal(row['Active'], datatype=XSD.int)))    \n",
    "    g.add((Day, LAO['hasNOfDeaths'], Literal(row['Deaths'], datatype=XSD.int)))    \n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('covidDays.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   CITY\n",
      "ZIP CODE               \n",
      "90028       LOS ANGELES\n",
      "91311        CHATSWORTH\n",
      "91402     PANORAMA CITY\n",
      "90025       LOS ANGELES\n",
      "90058            VERNON\n",
      "...                 ...\n",
      "90290           TOPANGA\n",
      "91501           BURBANK\n",
      "90032       LOS ANGELES\n",
      "90210     BEVERLY HILLS\n",
      "90037       LOS ANGELES\n",
      "\n",
      "[100 rows x 1 columns]\n",
      "603\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "cities1 = pd.read_csv(closedBusinessesData, sep=',', index_col='LOCATION ACCOUNT #')\n",
    "cities1 = cities1[['CITY','ZIP CODE']]\n",
    "\n",
    "\n",
    "cities2 = pd.read_csv(activeBusinessesData, sep=',', index_col='LOCATION ACCOUNT #')\n",
    "cities2 = cities2[['CITY','ZIP CODE']]\n",
    "\n",
    "\n",
    "cities = pd.merge(cities1, cities2)\n",
    "cities[\"ZIP CODE\"] = cities[\"ZIP CODE\"].str.split(\"-\", expand=True).get(0)\n",
    "cities = cities[cities['ZIP CODE'] != '']\n",
    "cities = cities.drop_duplicates()\n",
    "cities.set_index(\"ZIP CODE\", inplace=True)\n",
    "\n",
    "print(cities.head(100))\n",
    "print(len(cities))\n",
    "\n",
    "cities.to_csv('cities.csv', index=True)\n",
    "\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 109 ms\n",
      "Wall time: 175 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the league dataframe\n",
    "for index, row in cities.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = \"city\" + str(index)\n",
    "    City = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((City, RDF.type, LAO.City))\n",
    "    g.add((City, LAO['cityZipCode'], Literal(str(index), datatype=XSD.string)))    \n",
    "    g.add((City, LAO['cityName'], Literal(row['CITY'], datatype=XSD.string)))    \n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('cities.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Title\n",
      "Code                                              \n",
      "11      Agriculture, Forestry, Fishing and Hunting\n",
      "111                                Crop Production\n",
      "1111                     Oilseed and Grain Farming\n",
      "11111                              Soybean Farming\n",
      "111110                             Soybean Farming\n",
      "11112             Oilseed (except Soybean) Farming\n",
      "111120            Oilseed (except Soybean) Farming\n",
      "11113                     Dry Pea and Bean Farming\n",
      "111130                    Dry Pea and Bean Farming\n",
      "11114                                Wheat Farming\n",
      "111140                               Wheat Farming\n",
      "11115                                 Corn Farming\n",
      "111150                                Corn Farming\n",
      "11116                                 Rice Farming\n",
      "111160                                Rice Farming\n",
      "11119                          Other Grain Farming\n",
      "111191       Oilseed and Grain Combination Farming\n",
      "111199                     All Other Grain Farming\n",
      "1112                   Vegetable and Melon Farming\n",
      "11121                  Vegetable and Melon Farming\n",
      "2125\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "naics = pd.read_csv(naicsData, sep=',', index_col='Code')\n",
    "naics = naics[['Title']]\n",
    "naics[\"Title\"] = naics[\"Title\"].replace(\"T$\", \"\", regex=True)\n",
    "print(naics.head(20))\n",
    "print(len(naics))\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 219 ms\n",
      "Wall time: 423 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the league dataframe\n",
    "for index, row in naics.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = \"naics\" + str(index)\n",
    "    Naics = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Naics, RDF.type, LAO.Naics))\n",
    "    g.add((Naics, LAO['naicsCode'], Literal(index, datatype=XSD.string)))    \n",
    "    g.add((Naics, LAO['naicsDescription'], Literal(row['Title'], datatype=XSD.string)))    \n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('naics.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "areasList = ['Harbor Gateway-HAR', 'Palms - Mar Vista - Del Rey-PLM', 'Bel Air - Beverly Crest-BAR', 'Granada Hills - Knollwood-GHL', 'Mission Hills - Panorama City - North Hills-MSS', 'Encino - Tarzana-ENC', 'Brentwood - Pacific Palisades-BTW', 'West Adams - Baldwin Hills - Leimert-WAD', 'North Hollywood - Valley Village-NHL', 'Arleta - Pacoima-ARL', 'Northeast Los Angeles-NLA', 'Venice-VEN', 'Silver Lake - Echo Park - Elysian Valley-SLK', 'San Pedro-SPD', 'Reseda - West Van Nuys-RES', 'Sun Valley - La Tuna Canyon-SVY', 'Sunland - Tujunga - Lake View Terrace - Shadow Hills - East La Tuna Canyon-SLD', 'Westwood-WWD', 'West Los Angeles-WLA', 'Hollywood-HWD', 'Canoga Park - Winnetka - Woodland Hills - West Hills-CPK', 'Central City North-CCN', 'Chatsworth - Porter Ranch-CHT', 'Wilmington - Harbor City-WLM', 'Sylmar-SYL', 'Wilshire-WIL', 'Central City-CCY', 'Westlake-WLK', 'Port of Los Angeles-PTL', 'Northridge-NRD', 'Van Nuys - North Sherman Oaks-VNY', 'Boyle Heights-BHT', 'Sherman Oaks - Studio City - Toluca Lake - Cahuenga Pass-SHR', 'Westchester - Playa del Rey-WCH', 'Los Angeles International Airport-LAX', 'South Los Angeles-SLA', 'Southeast Los Angeles-SEL']\n",
    "# Split each string in the list into two parts, before and after the dash.\n",
    "city_names = [string.split('-')[0] for string in areasList]\n",
    "acronyms = [string.split('-')[-1] for string in areasList]\n",
    "\n",
    "# Create a DataFrame with the two columns.\n",
    "areas = pd.DataFrame({'CITY NAME': city_names, 'ACRONYM': acronyms})\n",
    "\n",
    "areas.set_index(\"ACRONYM\", inplace=True)\n",
    "\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"lao\", LAO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 9.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the league dataframe\n",
    "for index, row in areas.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = \"area\" + str(index)\n",
    "    Area = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "\n",
    "    #TYPE\n",
    "    g.add((Area, RDF.type, LAO.Area))\n",
    "    \n",
    "    #DATA PROPERTIES\n",
    "    g.add((Area, LAO['areaAcronym'], Literal(str(index), datatype=XSD.string)))    \n",
    "    g.add((Area, LAO['areaName'], Literal(row['CITY NAME'], datatype=XSD.string)))    \n",
    "\n",
    "    #OBJECT PROPERTIES\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('areas.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STREET ADDRESS</th>\n",
       "      <th>COMMUNITY PLANNING AREA</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHIAVE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lat33.7901lon-118.2804</th>\n",
       "      <td>1330 W PACIFIC COAST HIGHWAY SUITE #E</td>\n",
       "      <td>WLM</td>\n",
       "      <td>33.7901</td>\n",
       "      <td>-118.2804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lat33.9603lon-118.4471</th>\n",
       "      <td>327 CULVER BLVD UNIT #4</td>\n",
       "      <td>WCH</td>\n",
       "      <td>33.9603</td>\n",
       "      <td>-118.4471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lat34.1468lon-118.423</th>\n",
       "      <td>13317 VENTURA BLVD    #B</td>\n",
       "      <td>SHR</td>\n",
       "      <td>34.1468</td>\n",
       "      <td>-118.423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lat32.6665lon-117.1073</th>\n",
       "      <td>319 W 18TH STREET</td>\n",
       "      <td>_NOT FOUND_</td>\n",
       "      <td>32.6665</td>\n",
       "      <td>-117.1073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lat33.8301lon-118.328</th>\n",
       "      <td>1740   CRENSHAW BLVD</td>\n",
       "      <td>_NOT FOUND_</td>\n",
       "      <td>33.8301</td>\n",
       "      <td>-118.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lat33.9511lon-118.2739</th>\n",
       "      <td>94TH</td>\n",
       "      <td>SEL</td>\n",
       "      <td>33.9511</td>\n",
       "      <td>-118.2739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lat34.2687lon-118.3107</th>\n",
       "      <td>NASSAU                       AV</td>\n",
       "      <td>SLD</td>\n",
       "      <td>34.2687</td>\n",
       "      <td>-118.3107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lat34.069lon-118.3041</th>\n",
       "      <td>W  3RD                          ST</td>\n",
       "      <td>WIL</td>\n",
       "      <td>34.069</td>\n",
       "      <td>-118.3041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lat34.1536lon-118.4181</th>\n",
       "      <td>MILBANK                      ST</td>\n",
       "      <td>SHR</td>\n",
       "      <td>34.1536</td>\n",
       "      <td>-118.4181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lat33.9456lon-118.2323</th>\n",
       "      <td>FIGUEROA</td>\n",
       "      <td>SEL</td>\n",
       "      <td>33.9456</td>\n",
       "      <td>-118.2323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>282172 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               STREET ADDRESS  \\\n",
       "CHIAVE                                                          \n",
       "lat33.7901lon-118.2804  1330 W PACIFIC COAST HIGHWAY SUITE #E   \n",
       "lat33.9603lon-118.4471                327 CULVER BLVD UNIT #4   \n",
       "lat34.1468lon-118.423                13317 VENTURA BLVD    #B   \n",
       "lat32.6665lon-117.1073                      319 W 18TH STREET   \n",
       "lat33.8301lon-118.328                    1740   CRENSHAW BLVD   \n",
       "...                                                       ...   \n",
       "lat33.9511lon-118.2739                                   94TH   \n",
       "lat34.2687lon-118.3107        NASSAU                       AV   \n",
       "lat34.069lon-118.3041      W  3RD                          ST   \n",
       "lat34.1536lon-118.4181        MILBANK                      ST   \n",
       "lat33.9456lon-118.2323                               FIGUEROA   \n",
       "\n",
       "                       COMMUNITY PLANNING AREA LATITUDE  LONGITUDE  \n",
       "CHIAVE                                                              \n",
       "lat33.7901lon-118.2804                     WLM  33.7901  -118.2804  \n",
       "lat33.9603lon-118.4471                     WCH  33.9603  -118.4471  \n",
       "lat34.1468lon-118.423                      SHR  34.1468   -118.423  \n",
       "lat32.6665lon-117.1073             _NOT FOUND_  32.6665  -117.1073  \n",
       "lat33.8301lon-118.328              _NOT FOUND_  33.8301   -118.328  \n",
       "...                                        ...      ...        ...  \n",
       "lat33.9511lon-118.2739                     SEL  33.9511  -118.2739  \n",
       "lat34.2687lon-118.3107                     SLD  34.2687  -118.3107  \n",
       "lat34.069lon-118.3041                      WIL   34.069  -118.3041  \n",
       "lat34.1536lon-118.4181                     SHR  34.1536  -118.4181  \n",
       "lat33.9456lon-118.2323                     SEL  33.9456  -118.2323  \n",
       "\n",
       "[282172 rows x 4 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "activeBusinesses = pd.read_csv(activeBusinessesData, sep=',')\n",
    "closedBusinesses = pd.read_csv(closedBusinessesData, sep=',')\n",
    "crimeData1_df = pd.read_csv(crimeData1, sep=',')\n",
    "crimeData2_df = pd.read_csv(crimeData2, sep=',')\n",
    "crimeData3_df = pd.read_csv(crimeData3, sep=',')\n",
    "\n",
    "activeBusinesses = activeBusinesses[[\"STREET ADDRESS\", \"LOCATION\", \"COMMUNITY PLANNING AREA\"]]\n",
    "closedBusinesses = closedBusinesses[[\"STREET ADDRESS\", \"LOCATION\", \"COMMUNITY PLANNING AREA\"]]\n",
    "crimeData1_df = crimeData1_df[[\"LOCATION\", \"LAT\", \"LON\", \"COMMUNITY PLANNING AREA\"]]\n",
    "crimeData1_df.rename(columns={\"LOCATION\": \"STREET ADDRESS\", \"LAT\": \"LATITUDE\", \"LON\": \"LONGITUDE\", \"COMMUNITY PLANNING AREA\": \"COMMUNITY PLANNING AREA\"}, inplace=True)\n",
    "crimeData2_df = crimeData2_df[[\"LOCATION\", \"LAT\", \"LON\", \"COMMUNITY PLANNING AREA\"]]\n",
    "crimeData2_df.rename(columns={\"LOCATION\": \"STREET ADDRESS\", \"LAT\": \"LATITUDE\", \"LON\": \"LONGITUDE\", \"COMMUNITY PLANNING AREA\": \"COMMUNITY PLANNING AREA\"}, inplace=True)\n",
    "crimeData3_df = crimeData3_df[[\"LOCATION\", \"LAT\", \"LON\", \"COMMUNITY PLANNING AREA\"]]\n",
    "crimeData3_df.rename(columns={\"LOCATION\": \"STREET ADDRESS\", \"LAT\": \"LATITUDE\", \"LON\": \"LONGITUDE\", \"COMMUNITY PLANNING AREA\": \"COMMUNITY PLANNING AREA\"}, inplace=True)\n",
    "\n",
    "\n",
    "locations = pd.concat([activeBusinesses, closedBusinesses]).drop_duplicates()\n",
    "locations['LOCATION'] = locations['LOCATION'].str.replace('(','') \n",
    "locations['LOCATION'] = locations['LOCATION'].str.replace(')','')\n",
    "locations['LOCATION'] = locations['LOCATION'].str.replace(' ','')\n",
    "locations[['LATITUDE', 'LONGITUDE']] = locations['LOCATION'].str.split(',', expand=True)\n",
    "locations.drop(columns=['LOCATION'], inplace=True)\n",
    "# Remove the original coordinates column\n",
    "#locations.drop('LOCATION', axis=1, inplace=True)\n",
    "\n",
    "locations = pd.concat([locations, crimeData1_df, crimeData2_df, crimeData3_df]).drop_duplicates()\n",
    "locations['CHIAVE'] = \"lat\" + locations['LATITUDE'].astype(\"string\") + \"lon\" + locations['LONGITUDE'].astype(\"string\")\n",
    "locations.drop(locations[locations['CHIAVE'] == '0.0#0.0'].index, inplace = True)\n",
    "locations.set_index('CHIAVE', inplace=True)\n",
    "locations = locations.dropna()\n",
    "#0.0,0.0\n",
    "\n",
    "locations['COMMUNITY PLANNING AREA'] = locations['COMMUNITY PLANNING AREA'].str.split('-')\n",
    "get_last_element = lambda x: x[-1]\n",
    "locations['COMMUNITY PLANNING AREA'] = locations['COMMUNITY PLANNING AREA'].apply(get_last_element)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print((locations['COMMUNITY PLANNING AREA'].str.split('-').astype(\"string\")))\n",
    "\n",
    "\n",
    "\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 19 s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"lao\", LAO)\n",
    "\n",
    "#iterate over the club dataframe\n",
    "for index, row in locations.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the club id as URI\n",
    "    idU = index\n",
    "    Location = URIRef(LAO[idU])  # Corrected: Use LAO[idU] instead of LAO[idU]\n",
    "    # Add triples using the store's add() method.\n",
    "    g.add((Location, RDF.type, LAO.Location))\n",
    "\n",
    "    # DATA PROPERTIES\n",
    "    g.add((Location, LAO['hasLatitude'], Literal(row['LATITUDE'], datatype=XSD.double)))\n",
    "    g.add((Location, LAO['hasLongitude'], Literal(row['LONGITUDE'], datatype=XSD.double)))\n",
    "    g.add((Location, LAO['hasAddress'], Literal(row['STREET ADDRESS'], datatype=XSD.string)))\n",
    "\n",
    "    # OBJECT PROPERTIES\n",
    "    if (row[\"COMMUNITY PLANNING AREA\"] != '_NOT FOUND_'):\n",
    "        g.add((Location, LAO['belongsToArea'], LAO[\"area\" + str(row[\"COMMUNITY PLANNING AREA\"])]))\n",
    "\n",
    "\n",
    "    \n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open('locations.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>BUSINESS NAME</th>\n",
       "      <th>DBA NAME</th>\n",
       "      <th>STREET ADDRESS</th>\n",
       "      <th>CITY</th>\n",
       "      <th>ZIP CODE</th>\n",
       "      <th>LOCATION DESCRIPTION</th>\n",
       "      <th>MAILING ADDRESS</th>\n",
       "      <th>MAILING CITY</th>\n",
       "      <th>MAILING ZIP CODE</th>\n",
       "      <th>NAICS</th>\n",
       "      <th>PRIMARY NAICS DESCRIPTION</th>\n",
       "      <th>COUNCIL DISTRICT</th>\n",
       "      <th>LOCATION START DATE</th>\n",
       "      <th>LOCATION END DATE</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>COMMUNITY PLANNING AREA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOCATION ACCOUNT #</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0003176540-0001-1</th>\n",
       "      <td>0</td>\n",
       "      <td>SOEUNG CHAING</td>\n",
       "      <td>CAFECAFE</td>\n",
       "      <td>1330 W PACIFIC COAST HIGHWAY SUITE #E</td>\n",
       "      <td>WILMINGTON</td>\n",
       "      <td>90744-2467</td>\n",
       "      <td>1330 PACIFIC COAST 90744-2467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>722211.0</td>\n",
       "      <td>Limited-service eating places</td>\n",
       "      <td>15</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(33.7901, -118.2804)</td>\n",
       "      <td>Wilmington - Harbor City-WLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002973746-0002-1</th>\n",
       "      <td>1</td>\n",
       "      <td>BELINDA MIXON-JOY</td>\n",
       "      <td>BOOM'S ROOM</td>\n",
       "      <td>327 CULVER BLVD UNIT #4</td>\n",
       "      <td>PLAYA DEL REY</td>\n",
       "      <td>90293-7770</td>\n",
       "      <td>327 CULVER 90293-7770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>448150.0</td>\n",
       "      <td>Clothing accessories stores</td>\n",
       "      <td>11</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(33.9603, -118.4471)</td>\n",
       "      <td>Westchester - Playa del Rey-WCH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003318526-0001-9</th>\n",
       "      <td>2</td>\n",
       "      <td>MARTIN HERNANDEZ</td>\n",
       "      <td>E H PAINTING</td>\n",
       "      <td>1421 PICO STREET</td>\n",
       "      <td>SAN FERNANDO</td>\n",
       "      <td>91340-3506</td>\n",
       "      <td>1421 PICO 91340-3506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>233210.0</td>\n",
       "      <td>Single Family Housing Construction (1997 NAICS)</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>_INVALID_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003314268-0001-9</th>\n",
       "      <td>3</td>\n",
       "      <td>AZNIV SIMONYAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13317 VENTURA BLVD    #B</td>\n",
       "      <td>SHERMAN OAKS</td>\n",
       "      <td>91423-6210</td>\n",
       "      <td>13317 Ventura 91423-6210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>812112.0</td>\n",
       "      <td>Beauty salons</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(34.1468, -118.423)</td>\n",
       "      <td>Sherman Oaks - Studio City - Toluca Lake - Cah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003260354-0001-2</th>\n",
       "      <td>4</td>\n",
       "      <td>SOCAL DEMOLITION COMPANY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>319 W 18TH STREET</td>\n",
       "      <td>NATIONAL CITY</td>\n",
       "      <td>91950-5525</td>\n",
       "      <td>319 18TH 91950</td>\n",
       "      <td>319 W 18TH STREET</td>\n",
       "      <td>NATIONAL CITY</td>\n",
       "      <td>91950-5525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(32.6665, -117.1073)</td>\n",
       "      <td>_NOT FOUND_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003230029-0001-1</th>\n",
       "      <td>143047</td>\n",
       "      <td>AMETHYST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11469   MOORPARK STREET   APT #1</td>\n",
       "      <td>NORTH HOLLYWOOD</td>\n",
       "      <td>91602-2045</td>\n",
       "      <td>11469 MOORPARK 91602</td>\n",
       "      <td>10822 OTSEGO STREET APT #102</td>\n",
       "      <td>NORTH HOLLYWOOD</td>\n",
       "      <td>91601-3957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>(34.1507, -118.3816)</td>\n",
       "      <td>Sherman Oaks - Studio City - Toluca Lake - Cah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000876436-0003-8</th>\n",
       "      <td>143048</td>\n",
       "      <td>YOJANA N ZAPATA</td>\n",
       "      <td>HAPPY DOG GROOMING # 2</td>\n",
       "      <td>7621   WOODLAKE AVENUE</td>\n",
       "      <td>WEST HILLS</td>\n",
       "      <td>91304-5327</td>\n",
       "      <td>7621 WOODLAKE 91304-5327</td>\n",
       "      <td>16201   VICTORY BLVD</td>\n",
       "      <td>VAN NUYS</td>\n",
       "      <td>91406-5821</td>\n",
       "      <td>812910.0</td>\n",
       "      <td>Pet care (except veterinary) services</td>\n",
       "      <td>12</td>\n",
       "      <td>2017-06-15</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>(34.2088, -118.6323)</td>\n",
       "      <td>Canoga Park - Winnetka - Woodland Hills - West...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002632808-0001-6</th>\n",
       "      <td>143049</td>\n",
       "      <td>SEAN REILLY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3568 KEYSTONE AVENUE #7</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>90034-5569</td>\n",
       "      <td>3568 KEYSTONE 90034-5569</td>\n",
       "      <td>1380 W CAPITOL DRIVE UNIT #115</td>\n",
       "      <td>SAN PEDRO</td>\n",
       "      <td>90732-5082</td>\n",
       "      <td>541600.0</td>\n",
       "      <td>Management, scientific, &amp; technical consulting...</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-08-31</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>(34.0242, -118.4098)</td>\n",
       "      <td>Palms - Mar Vista - Del Rey-PLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003052169-0001-0</th>\n",
       "      <td>143050</td>\n",
       "      <td>JENNIFER R. UTULO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20649   SKOURAS DRIVE</td>\n",
       "      <td>WINNETKA</td>\n",
       "      <td>91306-4044</td>\n",
       "      <td>20649 SKOURAS 91306-4044</td>\n",
       "      <td>POST OFFICE BOX #5444</td>\n",
       "      <td>WEST HILLS</td>\n",
       "      <td>91308-5444</td>\n",
       "      <td>812990.0</td>\n",
       "      <td>All other personal services</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-02-25</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>(34.1928, -118.5836)</td>\n",
       "      <td>Canoga Park - Winnetka - Woodland Hills - West...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000840560-0001-1</th>\n",
       "      <td>143051</td>\n",
       "      <td>STREET WISE INVESTMENTS LLC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640 S MAIN STREET</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>90014-2004</td>\n",
       "      <td>640 MAIN 90014-2004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>721100.0</td>\n",
       "      <td>Traveler accommodation (including hotels, mote...</td>\n",
       "      <td>14</td>\n",
       "      <td>1998-04-10</td>\n",
       "      <td>2030-07-02</td>\n",
       "      <td>(34.0447, -118.2504)</td>\n",
       "      <td>Central City-CCY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260355 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Unnamed: 0                BUSINESS NAME  \\\n",
       "LOCATION ACCOUNT #                                            \n",
       "0003176540-0001-1            0                SOEUNG CHAING   \n",
       "0002973746-0002-1            1            BELINDA MIXON-JOY   \n",
       "0003318526-0001-9            2             MARTIN HERNANDEZ   \n",
       "0003314268-0001-9            3               AZNIV SIMONYAN   \n",
       "0003260354-0001-2            4     SOCAL DEMOLITION COMPANY   \n",
       "...                        ...                          ...   \n",
       "0003230029-0001-1       143047                     AMETHYST   \n",
       "0000876436-0003-8       143048              YOJANA N ZAPATA   \n",
       "0002632808-0001-6       143049                  SEAN REILLY   \n",
       "0003052169-0001-0       143050            JENNIFER R. UTULO   \n",
       "0000840560-0001-1       143051  STREET WISE INVESTMENTS LLC   \n",
       "\n",
       "                                  DBA NAME  \\\n",
       "LOCATION ACCOUNT #                           \n",
       "0003176540-0001-1                 CAFECAFE   \n",
       "0002973746-0002-1              BOOM'S ROOM   \n",
       "0003318526-0001-9             E H PAINTING   \n",
       "0003314268-0001-9                      NaN   \n",
       "0003260354-0001-2                      NaN   \n",
       "...                                    ...   \n",
       "0003230029-0001-1                      NaN   \n",
       "0000876436-0003-8   HAPPY DOG GROOMING # 2   \n",
       "0002632808-0001-6                      NaN   \n",
       "0003052169-0001-0                      NaN   \n",
       "0000840560-0001-1                      NaN   \n",
       "\n",
       "                                           STREET ADDRESS             CITY  \\\n",
       "LOCATION ACCOUNT #                                                           \n",
       "0003176540-0001-1   1330 W PACIFIC COAST HIGHWAY SUITE #E       WILMINGTON   \n",
       "0002973746-0002-1                 327 CULVER BLVD UNIT #4    PLAYA DEL REY   \n",
       "0003318526-0001-9                        1421 PICO STREET     SAN FERNANDO   \n",
       "0003314268-0001-9                13317 VENTURA BLVD    #B     SHERMAN OAKS   \n",
       "0003260354-0001-2                       319 W 18TH STREET    NATIONAL CITY   \n",
       "...                                                   ...              ...   \n",
       "0003230029-0001-1        11469   MOORPARK STREET   APT #1  NORTH HOLLYWOOD   \n",
       "0000876436-0003-8                  7621   WOODLAKE AVENUE       WEST HILLS   \n",
       "0002632808-0001-6                 3568 KEYSTONE AVENUE #7      LOS ANGELES   \n",
       "0003052169-0001-0                   20649   SKOURAS DRIVE         WINNETKA   \n",
       "0000840560-0001-1                       640 S MAIN STREET      LOS ANGELES   \n",
       "\n",
       "                      ZIP CODE           LOCATION DESCRIPTION  \\\n",
       "LOCATION ACCOUNT #                                              \n",
       "0003176540-0001-1   90744-2467  1330 PACIFIC COAST 90744-2467   \n",
       "0002973746-0002-1   90293-7770          327 CULVER 90293-7770   \n",
       "0003318526-0001-9   91340-3506           1421 PICO 91340-3506   \n",
       "0003314268-0001-9   91423-6210       13317 Ventura 91423-6210   \n",
       "0003260354-0001-2   91950-5525                 319 18TH 91950   \n",
       "...                        ...                            ...   \n",
       "0003230029-0001-1   91602-2045           11469 MOORPARK 91602   \n",
       "0000876436-0003-8   91304-5327       7621 WOODLAKE 91304-5327   \n",
       "0002632808-0001-6   90034-5569       3568 KEYSTONE 90034-5569   \n",
       "0003052169-0001-0   91306-4044       20649 SKOURAS 91306-4044   \n",
       "0000840560-0001-1   90014-2004            640 MAIN 90014-2004   \n",
       "\n",
       "                                   MAILING ADDRESS     MAILING CITY  \\\n",
       "LOCATION ACCOUNT #                                                    \n",
       "0003176540-0001-1                              NaN              NaN   \n",
       "0002973746-0002-1                              NaN              NaN   \n",
       "0003318526-0001-9                              NaN              NaN   \n",
       "0003314268-0001-9                              NaN              NaN   \n",
       "0003260354-0001-2                319 W 18TH STREET    NATIONAL CITY   \n",
       "...                                            ...              ...   \n",
       "0003230029-0001-1     10822 OTSEGO STREET APT #102  NORTH HOLLYWOOD   \n",
       "0000876436-0003-8             16201   VICTORY BLVD         VAN NUYS   \n",
       "0002632808-0001-6   1380 W CAPITOL DRIVE UNIT #115        SAN PEDRO   \n",
       "0003052169-0001-0            POST OFFICE BOX #5444       WEST HILLS   \n",
       "0000840560-0001-1                              NaN              NaN   \n",
       "\n",
       "                   MAILING ZIP CODE     NAICS  \\\n",
       "LOCATION ACCOUNT #                              \n",
       "0003176540-0001-1               NaN  722211.0   \n",
       "0002973746-0002-1               NaN  448150.0   \n",
       "0003318526-0001-9               NaN  233210.0   \n",
       "0003314268-0001-9               NaN  812112.0   \n",
       "0003260354-0001-2        91950-5525       NaN   \n",
       "...                             ...       ...   \n",
       "0003230029-0001-1        91601-3957       NaN   \n",
       "0000876436-0003-8        91406-5821  812910.0   \n",
       "0002632808-0001-6        90732-5082  541600.0   \n",
       "0003052169-0001-0        91308-5444  812990.0   \n",
       "0000840560-0001-1               NaN  721100.0   \n",
       "\n",
       "                                            PRIMARY NAICS DESCRIPTION  \\\n",
       "LOCATION ACCOUNT #                                                      \n",
       "0003176540-0001-1                       Limited-service eating places   \n",
       "0002973746-0002-1                         Clothing accessories stores   \n",
       "0003318526-0001-9     Single Family Housing Construction (1997 NAICS)   \n",
       "0003314268-0001-9                                       Beauty salons   \n",
       "0003260354-0001-2                                                 NaN   \n",
       "...                                                               ...   \n",
       "0003230029-0001-1                                                 NaN   \n",
       "0000876436-0003-8               Pet care (except veterinary) services   \n",
       "0002632808-0001-6   Management, scientific, & technical consulting...   \n",
       "0003052169-0001-0                         All other personal services   \n",
       "0000840560-0001-1   Traveler accommodation (including hotels, mote...   \n",
       "\n",
       "                    COUNCIL DISTRICT LOCATION START DATE LOCATION END DATE  \\\n",
       "LOCATION ACCOUNT #                                                           \n",
       "0003176540-0001-1                 15          2020-01-01               NaN   \n",
       "0002973746-0002-1                 11          2021-05-12               NaN   \n",
       "0003318526-0001-9                  0          2022-06-01               NaN   \n",
       "0003314268-0001-9                  4          2022-05-09               NaN   \n",
       "0003260354-0001-2                  0          2021-04-01               NaN   \n",
       "...                              ...                 ...               ...   \n",
       "0003230029-0001-1                  2          2020-05-04        2023-12-31   \n",
       "0000876436-0003-8                 12          2017-06-15        2023-12-31   \n",
       "0002632808-0001-6                  5          2011-08-31        2023-12-31   \n",
       "0003052169-0001-0                  3          2017-02-25        2023-12-31   \n",
       "0000840560-0001-1                 14          1998-04-10        2030-07-02   \n",
       "\n",
       "                                LOCATION  \\\n",
       "LOCATION ACCOUNT #                         \n",
       "0003176540-0001-1   (33.7901, -118.2804)   \n",
       "0002973746-0002-1   (33.9603, -118.4471)   \n",
       "0003318526-0001-9                    NaN   \n",
       "0003314268-0001-9    (34.1468, -118.423)   \n",
       "0003260354-0001-2   (32.6665, -117.1073)   \n",
       "...                                  ...   \n",
       "0003230029-0001-1   (34.1507, -118.3816)   \n",
       "0000876436-0003-8   (34.2088, -118.6323)   \n",
       "0002632808-0001-6   (34.0242, -118.4098)   \n",
       "0003052169-0001-0   (34.1928, -118.5836)   \n",
       "0000840560-0001-1   (34.0447, -118.2504)   \n",
       "\n",
       "                                              COMMUNITY PLANNING AREA  \n",
       "LOCATION ACCOUNT #                                                     \n",
       "0003176540-0001-1                        Wilmington - Harbor City-WLM  \n",
       "0002973746-0002-1                     Westchester - Playa del Rey-WCH  \n",
       "0003318526-0001-9                                           _INVALID_  \n",
       "0003314268-0001-9   Sherman Oaks - Studio City - Toluca Lake - Cah...  \n",
       "0003260354-0001-2                                         _NOT FOUND_  \n",
       "...                                                               ...  \n",
       "0003230029-0001-1   Sherman Oaks - Studio City - Toluca Lake - Cah...  \n",
       "0000876436-0003-8   Canoga Park - Winnetka - Woodland Hills - West...  \n",
       "0002632808-0001-6                     Palms - Mar Vista - Del Rey-PLM  \n",
       "0003052169-0001-0   Canoga Park - Winnetka - Woodland Hills - West...  \n",
       "0000840560-0001-1                                    Central City-CCY  \n",
       "\n",
       "[260355 rows x 17 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "activeBusinesses = pd.read_csv(activeBusinessesData, sep=',', index_col='LOCATION ACCOUNT #')\n",
    "closedBusinesses = pd.read_csv(closedBusinessesData, sep=',', index_col='LOCATION ACCOUNT #')\n",
    "closedBusinesses['LOCATION START DATE'] = pd.to_datetime(closedBusinesses['LOCATION START DATE'], format='%m/%d/%Y').dt.strftime('%Y-%m-%d')\n",
    "closedBusinesses['LOCATION END DATE'] = pd.to_datetime(closedBusinesses['LOCATION END DATE'], format='%m/%d/%Y').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "businesses = pd.concat([activeBusinesses, closedBusinesses]).drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)\n",
    "\n",
    "businesses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 25.5 s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "businesses[\"FILTERED ZIP\"] = businesses[\"ZIP CODE\"].str.split(\"-\", expand=True).get(0)\n",
    "businesses[\"NAICS\"] = businesses[\"NAICS\"].astype(\"Int64\")\n",
    "#iterate over the league dataframe\n",
    "for index, row in businesses.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = str(index)\n",
    "    Business = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "\n",
    "    #TYPE\n",
    "    g.add((Business, RDF.type, LAO.Business))\n",
    "    \n",
    "    #DATA PROPERTIES\n",
    "    g.add((Business, LAO['businessId'], Literal(str(index), datatype=XSD.string)))    \n",
    "    g.add((Business, LAO['businessName'], Literal(row['BUSINESS NAME'], datatype=XSD.string)))\n",
    "    if (pd.isna(row[\"DBA NAME\"]) == False):    \n",
    "        g.add((Business, LAO['doingBusinessName'], Literal(row['DBA NAME'], datatype=XSD.string)))\n",
    "\n",
    "    #OBJECT PROPERTIES\n",
    "    if (pd.isna(row[\"LOCATION START DATE\"]) == False):\n",
    "        g.add((Business, LAO['openedOnDate'], LAO[\"day\" + str(row['LOCATION START DATE'])]))\n",
    "    if (pd.isna(row[\"LOCATION END DATE\"]) == False):\n",
    "        g.add((Business, LAO['closedOnDate'], LAO[\"day\" + str(row['LOCATION END DATE'])]))\n",
    "    if (row[\"FILTERED ZIP\"] != ''):\n",
    "        g.add((Business, LAO['locatedInCity'], LAO[\"city\" + str(urllib.parse.quote(row[\"FILTERED ZIP\"]))]))\n",
    "    if (pd.isna(row[\"NAICS\"]) == False):\n",
    "        g.add((Business, LAO['hasNaics'], LAO[\"naics\" + str(row['NAICS'])]))\n",
    "    if (pd.isna(row[\"LOCATION\"]) == False):\n",
    "        row['LOCATION'] = str(row['LOCATION']).replace('(','') \n",
    "        row['LOCATION'] = row['LOCATION'].replace(')','')\n",
    "        row['LOCATION'] = row['LOCATION'].replace(' ','')\n",
    "        lat = row['LOCATION'].split(',')[0]\n",
    "        lon = row['LOCATION'].split(',')[1]\n",
    "        coordsURI = 'lat' + lat + 'lon' + lon\n",
    "        g.add((Business, LAO['locatedIn'], LAO[str(coordsURI)]))\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('businesses.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "playersFifa = pd.read_csv(playersFifaUrl, sep=',', index_col='sofifa_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='Name', keep_default_na=False, na_values=['_'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the players dataframe\n",
    "for index, row in players.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the player id as URI\n",
    "    idU = \"player\"+str(index)\n",
    "    Player = URIRef(SO[idU])\n",
    "    # the transferMarkt profile has as URI, the URL of the profile in the website\n",
    "    TransfermarktProfile = URIRef(row['url'])\n",
    "    \n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Player, RDF.type, SO.Player))\n",
    "    g.add((TransfermarktProfile, RDF.type, SO.TransfermarktProfile))\n",
    "    g.add((TransfermarktProfile, SO['isAbout'], Player))\n",
    "    \n",
    "    #process player name\n",
    "    name = row['name'].split('-')\n",
    "\n",
    "    if (len(name)>1):\n",
    "        g.add((Player, SO['firstName'], Literal(name[0], datatype=XSD.string)))\n",
    "        g.add((Player, SO['lastName'], Literal(name[1], datatype=XSD.string)))\n",
    "    else:\n",
    "        g.add((Player, SO['lastName'], Literal(name[0], datatype=XSD.string)))\n",
    "        \n",
    "    #there can be more than one position per player\n",
    "    for pos in row['position'].split(' - '):\n",
    "        g.add((Player, SO['position'], Literal(pos.lower(), datatype=XSD.string)))\n",
    "    \n",
    "    if not(row['club_id']==''):\n",
    "        idC = \"club\"+str(row['club_id'])\n",
    "        g.add((Player, SO['playFor'], URIRef(SO[idC])))\n",
    "\n",
    "#iterate over the fifa dataframe\n",
    "for index, row in playersFifa.iterrows():\n",
    "    pname = row['short_name'].lower()\n",
    "    if ('.' in pname):\n",
    "        # get last name\n",
    "        # in the fifa dataset we have short names as L. Messi so we delete the L. \n",
    "        # we need to check if the last name contains a space\n",
    "        pname = row['short_name'].split('.')[1].lower().strip()\n",
    "        if ' ' in pname:\n",
    "            i = 0\n",
    "            for t in pname.split(' '):\n",
    "                if i == 0:\n",
    "                    pname = t.lower()\n",
    "                else:\n",
    "                    pname = pname + \"-\" + t.lower()\n",
    "                i += 1           \n",
    "    elif(' ' in pname):\n",
    "        # here we have to handle Cristiano Ronaldo mapping it to cristiano-ronaldo to maximize the match in the players dataframe \n",
    "        i = 0\n",
    "        for t in row['short_name'].split(' '):\n",
    "            if i == 0:\n",
    "                pname = t.lower()\n",
    "            else:\n",
    "                pname = pname + \"-\" + t.lower()\n",
    "            i += 1\n",
    "    pname = strip_accents(pname)\n",
    "    \n",
    "    # find sim with the full name \n",
    "    fullname = row['long_name'].lower()\n",
    "    i = 0\n",
    "    for t in fullname.split(' '):\n",
    "        if i == 0:\n",
    "            fullname = t.lower()\n",
    "        else:\n",
    "            fullname = fullname + \"-\" + t.lower()\n",
    "        i += 1 \n",
    "    fullname = strip_accents(fullname)\n",
    "    # check the players with that last name\n",
    "    names =  players[players['name'].str.contains(pname)]['name']\n",
    "    #find max similarity    \n",
    "    maxN = 0\n",
    "    playerId = ''\n",
    "    for n in names:\n",
    "        sim = SequenceMatcher(None, fullname, n).ratio()\n",
    "        if (maxN < sim):\n",
    "            maxN = sim\n",
    "            playerId = players.loc[players['name'] == n].index[0]\n",
    "        \n",
    "    #if we get a valid playerId we can connect the Fifa stats to the transfermrkt player\n",
    "    if (playerId != ''):\n",
    "        #remove the row from the player dataframe to avoid futher matchings (we know data will contain errors)\n",
    "        players = players.drop(index=playerId)\n",
    "        idU = \"player\"+str(playerId)\n",
    "        Player = URIRef(SO[idU])\n",
    "        g.add((Player, SO['overallFifaValue'], Literal(row['overall'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['growthFifaPotential'], Literal(row['potential'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['economicValue'], Literal(row['value_eur'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['annualWage'], Literal(row['wage_eur'], datatype=XSD.int))) \n",
    "        \n",
    "        pFeatures = str(row['player_tags'])\n",
    "        if pFeatures != '_' and pFeatures != '':\n",
    "            pFeatures = pFeatures.split(',')\n",
    "            for feature in pFeatures:\n",
    "                feature = feature.strip()\n",
    "                feature = re.sub('#', '', feature)\n",
    "                g.add((Player, SO['playerFeature'], Literal(feature, datatype=XSD.string)))\n",
    "        \n",
    "        if row['contract_valid_until'] != '_' and row['contract_valid_until'] != '':\n",
    "            g.add((Player, SO['contractValidTo'], Literal(int(row['contract_valid_until']), datatype=XSD.gYear)))        \n",
    "\n",
    "        g.add((Player, SO['birthday'], Literal(row['dob'], datatype=XSD.date)))\n",
    "        g.add((Player, SO['height'], Literal(row['height_cm'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['weight'], Literal(row['weight_kg'], datatype=XSD.int)))\n",
    "        \n",
    "        \n",
    "        nationality = row['nationality'] \n",
    "        nationality = nationality.replace(\" \", \"_\")\n",
    "        # create the RDF node\n",
    "        Country = URIRef(CNS[nationality])\n",
    "        # add the edge connecting the Movie and the Country \n",
    "        g.add((Player, SO['nationality'], Country))   \n",
    "\n",
    "        # Homework: extend the code to populate the 'propertyOf' edge\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'players.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "apps = pd.read_csv(appearancesUrl, sep=',', index_col='appearance_id', keep_default_na=False, na_values=['_'])\n",
    "games = pd.read_csv(gamesUrl, sep=',', index_col='game_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over the games dataframe\n",
    "for index, row in games.iterrows():\n",
    "    # we use the transfermrket URL as URI\n",
    "    Game = URIRef(row['url'])\n",
    "    g.add((Game, RDF.type, SO.Game))\n",
    "    idU1 = \"club\"+str(row['home_club_id'])\n",
    "    idU2 = \"club\"+str(row['away_club_id'])\n",
    "    HomeClub = URIRef(SO[idU1])\n",
    "    AwayClub = URIRef(SO[idU2])\n",
    "    g.add((Game, SO['homeClub'], HomeClub))\n",
    "    g.add((Game, SO['awayClub'], AwayClub))    \n",
    "    g.add((Game, SO['matchDay'], Literal(row['date'], datatype=XSD.date)))\n",
    "    g.add((Game, SO['homeClubGoals'], Literal(row['home_club_goals'], datatype=XSD.int)))\n",
    "    g.add((Game, SO['awayClubGoals'], Literal(row['away_club_goals'], datatype=XSD.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 4.7 s, sys: 55.8 ms, total: 4.75 s\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'games.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the full players dataframe\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldgameid = ''\n",
    "for index, row in apps.iterrows():\n",
    "    idA = \"appearance\"+str(index)\n",
    "    idP = \"player\"+str(row['player_id'])\n",
    "    idG = \"game\"+currgameid\n",
    "    Appearance = URIRef(SO[idA])\n",
    "    Player = URIRef(SO[idP])\n",
    "    currgameid = str(row['game_id'])\n",
    "    Game = URIRef(SO[idG])\n",
    "    g.add((Appearance, RDF.type, SO.Appearance))\n",
    "    g.add((Player, SO['appearIn'], Appearance))\n",
    "    g.add((Appearance, SO['playIn'], Game))\n",
    "\n",
    "    g.add((Appearance, SO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['minutesPlayed'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['yellowCard'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['redCard'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "\n",
    "    #add this triple only once per game\n",
    "    if (currgameid != oldgameid):\n",
    "        idL = \"league\"+str(row['league_id'])\n",
    "        g.add((Game, SO['belongTo'], URIRef(SO[idL])))\n",
    "        oldgameid = currgameid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 1min 22s, sys: 1.32 s, total: 1min 23s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'appearances.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
