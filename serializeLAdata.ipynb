{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the Soccer Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "# CHECK DATE \n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "activeBusinessesData = 'datasets/ACTIVE BUSINESSES/Listing_of_Active_Businesses_parsed.csv'\n",
    "closedBusinessesData = 'datasets/CLOSED BUSINESSES/All_Closed_Businesses_20231101.csv'\n",
    "laCovidData = 'datasets/COVID DATA/sorted_los_angeles_covid_data.csv'\n",
    "crimeData1 = 'datasets/CRIME DATA/Crime_Data_from_2020_to_Present_1.csv'\n",
    "crimeData2 = 'datasets/CRIME DATA/Crime_Data_from_2020_to_Present_2.csv'\n",
    "crimeData3 = 'datasets/CRIME DATA/Crime_Data_from_2020_to_Present_3.csv'\n",
    "crimeCodesDescData = 'datasets/CRIME DATA/CrimesCodesAndDesc_listed.csv'\n",
    "moCodesData = 'datasets/CRIME DATA/CrimesCodesAndDesc_listed.csv'\n",
    "weaponData = 'datasets/CRIME DATA/weapon_ds.csv'\n",
    "\n",
    "\n",
    "# saving folder\n",
    "savePath =  path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "#CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "LAO = Namespace(\"http://www.bitsei.it/losAngelesOntology/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              FIPS       Admin2 Province_State Country_Region  \\\n",
      "solodata                                                        \n",
      "2020-02-04  6037.0  Los Angeles     California             US   \n",
      "2020-03-22  6037.0  Los Angeles     California             US   \n",
      "2020-03-23  6037.0  Los Angeles     California             US   \n",
      "2020-03-24  6037.0  Los Angeles     California             US   \n",
      "2020-03-25  6037.0  Los Angeles     California             US   \n",
      "\n",
      "                   Last_Update        Lat       Long_  Confirmed  Deaths  \\\n",
      "solodata                                                                   \n",
      "2020-02-04 2020-02-04 23:25:00  34.308284 -118.228241       4045    78.0   \n",
      "2020-03-22 2020-03-22 23:45:00  34.308284 -118.228241        407     5.0   \n",
      "2020-03-23 2020-03-23 23:19:34  34.308284 -118.228241        536     7.0   \n",
      "2020-03-24 2020-03-24 23:37:31  34.308284 -118.228241        662    11.0   \n",
      "2020-03-25 2020-03-25 23:33:19  34.308284 -118.228241        812    13.0   \n",
      "\n",
      "            Recovered  Active                 Combined_Key  Incident_Rate  \\\n",
      "solodata                                                                    \n",
      "2020-02-04        0.0  3967.0  Los Angeles, California, US            NaN   \n",
      "2020-03-22        0.0   402.0  Los Angeles, California, US            NaN   \n",
      "2020-03-23        0.0   529.0  Los Angeles, California, US            NaN   \n",
      "2020-03-24        0.0   651.0  Los Angeles, California, US            NaN   \n",
      "2020-03-25        0.0   799.0  Los Angeles, California, US            NaN   \n",
      "\n",
      "            Case_Fatality_Ratio  Incidence_Rate  Case-Fatality_Ratio  \n",
      "solodata                                                              \n",
      "2020-02-04                  NaN             NaN                  NaN  \n",
      "2020-03-22                  NaN             NaN                  NaN  \n",
      "2020-03-23                  NaN             NaN                  NaN  \n",
      "2020-03-24                  NaN             NaN                  NaN  \n",
      "2020-03-25                  NaN             NaN                  NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "laCovid = pd.read_csv(laCovidData, sep=',')\n",
    "\n",
    "laCovid[\"Last_Update\"] = pd.to_datetime(laCovid['Last_Update'])\n",
    "laCovid['solodata'] = laCovid['Last_Update'].dt.date\n",
    "\n",
    "laCovid.set_index(\"solodata\", inplace=True)\n",
    "\n",
    "print(laCovid.head())\n",
    "\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 172 ms\n",
      "Wall time: 309 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the league dataframe\n",
    "for index, row in laCovid.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = str(index)\n",
    "    Day = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Day, RDF.type, LAO.Day))\n",
    "    g.add((Day, LAO['hasDate'], Literal(str(row['Last_Update']), datatype=XSD.datetime)))    \n",
    "    g.add((Day, LAO['hasActiveCases'], Literal(row['Active'], datatype=XSD.integer)))    \n",
    "    g.add((Day, LAO['hasNOfDeaths'], Literal(row['Deaths'], datatype=XSD.integer)))    \n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('covidDays.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('CITY', 'ZIP CODE')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ('CITY', 'ZIP CODE')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mirco\\Desktop\\Database 2\\bitsei-db2-unipd\\serializeLAdata.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mirco/Desktop/Database%202/bitsei-db2-unipd/serializeLAdata.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the CSV files in memory\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mirco/Desktop/Database%202/bitsei-db2-unipd/serializeLAdata.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m cities \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(closedBusinessesData, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, index_col\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLOCATION ACCOUNT #\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mirco/Desktop/Database%202/bitsei-db2-unipd/serializeLAdata.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m cities \u001b[39m=\u001b[39m cities[\u001b[39m'\u001b[39;49m\u001b[39mCITY\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mZIP CODE\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mirco/Desktop/Database%202/bitsei-db2-unipd/serializeLAdata.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(cities\u001b[39m.\u001b[39mhead())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mirco/Desktop/Database%202/bitsei-db2-unipd/serializeLAdata.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#create the graph\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: ('CITY', 'ZIP CODE')"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "cities = pd.read_csv(closedBusinessesData, sep=',', index_col='LOCATION ACCOUNT #')\n",
    "\n",
    "cities = cities['CITY','ZIP CODE']\n",
    "\n",
    "print(cities.head())\n",
    "\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "activeBusinesses = pd.read_csv(activeBusinessesData, sep=',', index_col='LOCATION ACCOUNT #')\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 4.97 s\n",
      "Wall time: 35.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the league dataframe\n",
    "for index, row in activeBusinesses.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = str(index)\n",
    "    Business = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Business, RDF.type, LAO.Business))\n",
    "    g.add((Business, LAO['businessId'], Literal(str(index), datatype=XSD.string)))    \n",
    "    g.add((Business, LAO['businessName'], Literal(row['BUSINESS NAME'], datatype=XSD.string)))    \n",
    "    g.add((Business, LAO['doingBusinessName'], Literal(row['DBA NAME'], datatype=XSD.string)))    \n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('activeBusinesses.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "clubs = pd.read_csv(clubsUrl, sep=',', index_col='club_id')\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96.4 ms, sys: 3.49 ms, total: 99.9 ms\n",
      "Wall time: 102 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the club dataframe\n",
    "for index, row in clubs.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the club id as URI\n",
    "    idU = \"club\"+str(index)\n",
    "    Club = URIRef(SO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Club, RDF.type, SO.Club))\n",
    "    g.add((Club, SO['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "    idL = \"league\"+str(row['league_id'])\n",
    "    g.add((Club, SO['competeIn'], URIRef(SO[idL])))\n",
    "    \n",
    "    try:\n",
    "        # get the nationality of the club\n",
    "        nationality = leagues.loc[row['league_id'], 'nationality' ]\n",
    "        # create the RDF node\n",
    "        Country = URIRef(CNS[row['nationality']])\n",
    "        # add the edge connecting the Movie and the Country \n",
    "        g.add((Club, SO['nationality'], Country))    \n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 74 ms, sys: 3.26 ms, total: 77.3 ms\n",
      "Wall time: 78.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'clubs.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "playersFifa = pd.read_csv(playersFifaUrl, sep=',', index_col='sofifa_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='Name', keep_default_na=False, na_values=['_'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the players dataframe\n",
    "for index, row in players.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the player id as URI\n",
    "    idU = \"player\"+str(index)\n",
    "    Player = URIRef(SO[idU])\n",
    "    # the transferMarkt profile has as URI, the URL of the profile in the website\n",
    "    TransfermarktProfile = URIRef(row['url'])\n",
    "    \n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Player, RDF.type, SO.Player))\n",
    "    g.add((TransfermarktProfile, RDF.type, SO.TransfermarktProfile))\n",
    "    g.add((TransfermarktProfile, SO['isAbout'], Player))\n",
    "    \n",
    "    #process player name\n",
    "    name = row['name'].split('-')\n",
    "\n",
    "    if (len(name)>1):\n",
    "        g.add((Player, SO['firstName'], Literal(name[0], datatype=XSD.string)))\n",
    "        g.add((Player, SO['lastName'], Literal(name[1], datatype=XSD.string)))\n",
    "    else:\n",
    "        g.add((Player, SO['lastName'], Literal(name[0], datatype=XSD.string)))\n",
    "        \n",
    "    #there can be more than one position per player\n",
    "    for pos in row['position'].split(' - '):\n",
    "        g.add((Player, SO['position'], Literal(pos.lower(), datatype=XSD.string)))\n",
    "    \n",
    "    if not(row['club_id']==''):\n",
    "        idC = \"club\"+str(row['club_id'])\n",
    "        g.add((Player, SO['playFor'], URIRef(SO[idC])))\n",
    "\n",
    "#iterate over the fifa dataframe\n",
    "for index, row in playersFifa.iterrows():\n",
    "    pname = row['short_name'].lower()\n",
    "    if ('.' in pname):\n",
    "        # get last name\n",
    "        # in the fifa dataset we have short names as L. Messi so we delete the L. \n",
    "        # we need to check if the last name contains a space\n",
    "        pname = row['short_name'].split('.')[1].lower().strip()\n",
    "        if ' ' in pname:\n",
    "            i = 0\n",
    "            for t in pname.split(' '):\n",
    "                if i == 0:\n",
    "                    pname = t.lower()\n",
    "                else:\n",
    "                    pname = pname + \"-\" + t.lower()\n",
    "                i += 1           \n",
    "    elif(' ' in pname):\n",
    "        # here we have to handle Cristiano Ronaldo mapping it to cristiano-ronaldo to maximize the match in the players dataframe \n",
    "        i = 0\n",
    "        for t in row['short_name'].split(' '):\n",
    "            if i == 0:\n",
    "                pname = t.lower()\n",
    "            else:\n",
    "                pname = pname + \"-\" + t.lower()\n",
    "            i += 1\n",
    "    pname = strip_accents(pname)\n",
    "    \n",
    "    # find sim with the full name \n",
    "    fullname = row['long_name'].lower()\n",
    "    i = 0\n",
    "    for t in fullname.split(' '):\n",
    "        if i == 0:\n",
    "            fullname = t.lower()\n",
    "        else:\n",
    "            fullname = fullname + \"-\" + t.lower()\n",
    "        i += 1 \n",
    "    fullname = strip_accents(fullname)\n",
    "    # check the players with that last name\n",
    "    names =  players[players['name'].str.contains(pname)]['name']\n",
    "    #find max similarity    \n",
    "    maxN = 0\n",
    "    playerId = ''\n",
    "    for n in names:\n",
    "        sim = SequenceMatcher(None, fullname, n).ratio()\n",
    "        if (maxN < sim):\n",
    "            maxN = sim\n",
    "            playerId = players.loc[players['name'] == n].index[0]\n",
    "        \n",
    "    #if we get a valid playerId we can connect the Fifa stats to the transfermrkt player\n",
    "    if (playerId != ''):\n",
    "        #remove the row from the player dataframe to avoid futher matchings (we know data will contain errors)\n",
    "        players = players.drop(index=playerId)\n",
    "        idU = \"player\"+str(playerId)\n",
    "        Player = URIRef(SO[idU])\n",
    "        g.add((Player, SO['overallFifaValue'], Literal(row['overall'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['growthFifaPotential'], Literal(row['potential'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['economicValue'], Literal(row['value_eur'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['annualWage'], Literal(row['wage_eur'], datatype=XSD.int))) \n",
    "        \n",
    "        pFeatures = str(row['player_tags'])\n",
    "        if pFeatures != '_' and pFeatures != '':\n",
    "            pFeatures = pFeatures.split(',')\n",
    "            for feature in pFeatures:\n",
    "                feature = feature.strip()\n",
    "                feature = re.sub('#', '', feature)\n",
    "                g.add((Player, SO['playerFeature'], Literal(feature, datatype=XSD.string)))\n",
    "        \n",
    "        if row['contract_valid_until'] != '_' and row['contract_valid_until'] != '':\n",
    "            g.add((Player, SO['contractValidTo'], Literal(int(row['contract_valid_until']), datatype=XSD.gYear)))        \n",
    "\n",
    "        g.add((Player, SO['birthday'], Literal(row['dob'], datatype=XSD.date)))\n",
    "        g.add((Player, SO['height'], Literal(row['height_cm'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['weight'], Literal(row['weight_kg'], datatype=XSD.int)))\n",
    "        \n",
    "        \n",
    "        nationality = row['nationality'] \n",
    "        nationality = nationality.replace(\" \", \"_\")\n",
    "        # create the RDF node\n",
    "        Country = URIRef(CNS[nationality])\n",
    "        # add the edge connecting the Movie and the Country \n",
    "        g.add((Player, SO['nationality'], Country))   \n",
    "\n",
    "        # Homework: extend the code to populate the 'propertyOf' edge\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'players.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "apps = pd.read_csv(appearancesUrl, sep=',', index_col='appearance_id', keep_default_na=False, na_values=['_'])\n",
    "games = pd.read_csv(gamesUrl, sep=',', index_col='game_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over the games dataframe\n",
    "for index, row in games.iterrows():\n",
    "    # we use the transfermrket URL as URI\n",
    "    Game = URIRef(row['url'])\n",
    "    g.add((Game, RDF.type, SO.Game))\n",
    "    idU1 = \"club\"+str(row['home_club_id'])\n",
    "    idU2 = \"club\"+str(row['away_club_id'])\n",
    "    HomeClub = URIRef(SO[idU1])\n",
    "    AwayClub = URIRef(SO[idU2])\n",
    "    g.add((Game, SO['homeClub'], HomeClub))\n",
    "    g.add((Game, SO['awayClub'], AwayClub))    \n",
    "    g.add((Game, SO['matchDay'], Literal(row['date'], datatype=XSD.date)))\n",
    "    g.add((Game, SO['homeClubGoals'], Literal(row['home_club_goals'], datatype=XSD.int)))\n",
    "    g.add((Game, SO['awayClubGoals'], Literal(row['away_club_goals'], datatype=XSD.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 4.7 s, sys: 55.8 ms, total: 4.75 s\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'games.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the full players dataframe\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldgameid = ''\n",
    "for index, row in apps.iterrows():\n",
    "    idA = \"appearance\"+str(index)\n",
    "    idP = \"player\"+str(row['player_id'])\n",
    "    idG = \"game\"+currgameid\n",
    "    Appearance = URIRef(SO[idA])\n",
    "    Player = URIRef(SO[idP])\n",
    "    currgameid = str(row['game_id'])\n",
    "    Game = URIRef(SO[idG])\n",
    "    g.add((Appearance, RDF.type, SO.Appearance))\n",
    "    g.add((Player, SO['appearIn'], Appearance))\n",
    "    g.add((Appearance, SO['playIn'], Game))\n",
    "\n",
    "    g.add((Appearance, SO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['minutesPlayed'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['yellowCard'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['redCard'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "\n",
    "    #add this triple only once per game\n",
    "    if (currgameid != oldgameid):\n",
    "        idL = \"league\"+str(row['league_id'])\n",
    "        g.add((Game, SO['belongTo'], URIRef(SO[idL])))\n",
    "        oldgameid = currgameid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 1min 22s, sys: 1.32 s, total: 1min 23s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'appearances.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
