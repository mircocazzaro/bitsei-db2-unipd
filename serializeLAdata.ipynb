{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the Loa Angeles Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "# CHECK DATE \n",
    "from datetime import datetime\n",
    "import urllib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "activeBusinessesData = 'datasets/ACTIVE BUSINESSES/Listing_of_Active_Businesses_parsed.csv'\n",
    "closedBusinessesData = 'datasets/CLOSED BUSINESSES/All_Closed_Businesses_20231101.csv'\n",
    "laCovidData = 'datasets/COVID DATA/sorted_los_angeles_covid_data.csv'\n",
    "crimeData1 = 'datasets/CRIME DATA/Crime_Data_from_2020_to_Present_1.csv'\n",
    "crimeData2 = 'datasets/CRIME DATA/Crime_Data_from_2020_to_Present_2.csv'\n",
    "crimeData3 = 'datasets/CRIME DATA/Crime_Data_from_2020_to_Present_3.csv'\n",
    "crimeCodesDescData = 'datasets/CRIME DATA/CrimesCodesAndDesc_listed.csv'\n",
    "moCodesData = 'datasets/CRIME DATA/CrimesCodesAndDesc_listed.csv'\n",
    "naicsData = 'datasets/CLOSED BUSINESSES/2022_NAICS_Descriptions.csv'\n",
    "weaponData = 'datasets/CRIME DATA/weapon_ds.csv'\n",
    "\n",
    "\n",
    "# saving folder\n",
    "savePath =  path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "#CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "LAO = Namespace(\"http://www.bitsei.it/losAngelesOntology/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              FIPS       Admin2 Province_State Country_Region  \\\n",
      "solodata                                                        \n",
      "2020-02-04  6037.0  Los Angeles     California             US   \n",
      "2020-03-22  6037.0  Los Angeles     California             US   \n",
      "2020-03-23  6037.0  Los Angeles     California             US   \n",
      "2020-03-24  6037.0  Los Angeles     California             US   \n",
      "2020-03-25  6037.0  Los Angeles     California             US   \n",
      "\n",
      "                   Last_Update        Lat       Long_  Confirmed  Deaths  \\\n",
      "solodata                                                                   \n",
      "2020-02-04 2020-02-04 23:25:00  34.308284 -118.228241       4045      78   \n",
      "2020-03-22 2020-03-22 23:45:00  34.308284 -118.228241        407       5   \n",
      "2020-03-23 2020-03-23 23:19:34  34.308284 -118.228241        536       7   \n",
      "2020-03-24 2020-03-24 23:37:31  34.308284 -118.228241        662      11   \n",
      "2020-03-25 2020-03-25 23:33:19  34.308284 -118.228241        812      13   \n",
      "\n",
      "            Recovered  Active                 Combined_Key  Incident_Rate  \\\n",
      "solodata                                                                    \n",
      "2020-02-04        0.0    3967  Los Angeles, California, US            NaN   \n",
      "2020-03-22        0.0     402  Los Angeles, California, US            NaN   \n",
      "2020-03-23        0.0     529  Los Angeles, California, US            NaN   \n",
      "2020-03-24        0.0     651  Los Angeles, California, US            NaN   \n",
      "2020-03-25        0.0     799  Los Angeles, California, US            NaN   \n",
      "\n",
      "            Case_Fatality_Ratio  Incidence_Rate  Case-Fatality_Ratio  \n",
      "solodata                                                              \n",
      "2020-02-04                  NaN             NaN                  NaN  \n",
      "2020-03-22                  NaN             NaN                  NaN  \n",
      "2020-03-23                  NaN             NaN                  NaN  \n",
      "2020-03-24                  NaN             NaN                  NaN  \n",
      "2020-03-25                  NaN             NaN                  NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "laCovid = pd.read_csv(laCovidData, sep=',') #, dtype={'Active':int, 'Deaths':int}\n",
    "\n",
    "laCovid[\"Last_Update\"] = pd.to_datetime(laCovid['Last_Update'])\n",
    "laCovid['solodata'] = laCovid['Last_Update'].dt.date\n",
    "laCovid['Active'] = laCovid['Active'].astype('Int64')\n",
    "laCovid['Deaths'] = laCovid['Deaths'].astype('Int64')\n",
    "\n",
    "laCovid.set_index(\"solodata\", inplace=True)\n",
    "\n",
    "print(laCovid.head())\n",
    "\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 195 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:11: UserWarning: Code: datetime is not defined in namespace XSD\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the league dataframe\n",
    "for index, row in laCovid.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = str(index)\n",
    "    Day = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Day, RDF.type, LAO.Day))\n",
    "    g.add((Day, LAO['hasDate'], Literal(str(row['Last_Update']), datatype=XSD.datetime)))    \n",
    "    g.add((Day, LAO['hasActiveCases'], Literal(row['Active'], datatype=XSD.int)))    \n",
    "    g.add((Day, LAO['hasNOfDeaths'], Literal(row['Deaths'], datatype=XSD.int)))    \n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('covidDays.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    CITY\n",
      "ZIP CODE                \n",
      "91344      GRANADA HILLS\n",
      "90015        LOS ANGELES\n",
      "91306           WINNETKA\n",
      "90020        LOS ANGELES\n",
      "91303        CANOGA PARK\n",
      "...                  ...\n",
      "90292     MARINA DEL REY\n",
      "90710        HARBOR CITY\n",
      "90247            GARDENA\n",
      "91436             ENCINO\n",
      "91411           VAN NUYS\n",
      "\n",
      "[100 rows x 1 columns]\n",
      "763\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "cities1 = pd.read_csv(closedBusinessesData, sep=',', index_col='LOCATION ACCOUNT #')\n",
    "cities1 = cities1[['CITY','ZIP CODE']]\n",
    "\n",
    "\n",
    "cities2 = pd.read_csv(activeBusinessesData, sep=',', index_col='LOCATION ACCOUNT #')\n",
    "cities2 = cities2[['CITY','ZIP CODE']]\n",
    "\n",
    "\n",
    "cities = pd.merge(cities1, cities2)\n",
    "cities[\"ZIP CODE\"] = cities[\"ZIP CODE\"].str.split(\"-\", expand=True).get(0)\n",
    "cities = cities[cities['ZIP CODE'] != '']\n",
    "cities = cities.drop_duplicates()\n",
    "cities.set_index(\"ZIP CODE\", inplace=True)\n",
    "\n",
    "print(cities.head(100))\n",
    "print(len(cities))\n",
    "\n",
    "cities.to_csv('cities.csv', index=True)\n",
    "\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 94.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the league dataframe\n",
    "for index, row in cities.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = str(index)\n",
    "    City = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((City, RDF.type, LAO.City))\n",
    "    g.add((City, LAO['cityZipCode'], Literal(str(index), datatype=XSD.string)))    \n",
    "    g.add((City, LAO['cityName'], Literal(row['CITY'], datatype=XSD.string)))    \n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('cities.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Title\n",
      "Code                                              \n",
      "11      Agriculture, Forestry, Fishing and Hunting\n",
      "111                                Crop Production\n",
      "1111                     Oilseed and Grain Farming\n",
      "11111                              Soybean Farming\n",
      "111110                             Soybean Farming\n",
      "11112             Oilseed (except Soybean) Farming\n",
      "111120            Oilseed (except Soybean) Farming\n",
      "11113                     Dry Pea and Bean Farming\n",
      "111130                    Dry Pea and Bean Farming\n",
      "11114                                Wheat Farming\n",
      "111140                               Wheat Farming\n",
      "11115                                 Corn Farming\n",
      "111150                                Corn Farming\n",
      "11116                                 Rice Farming\n",
      "111160                                Rice Farming\n",
      "11119                          Other Grain Farming\n",
      "111191       Oilseed and Grain Combination Farming\n",
      "111199                     All Other Grain Farming\n",
      "1112                   Vegetable and Melon Farming\n",
      "11121                  Vegetable and Melon Farming\n",
      "2125\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "naics = pd.read_csv(naicsData, sep=',', index_col='Code')\n",
    "naics = naics[['Title']]\n",
    "naics[\"Title\"] = naics[\"Title\"].replace(\"T$\", \"\", regex=True)\n",
    "print(naics.head(20))\n",
    "print(len(naics))\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 259 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the league dataframe\n",
    "for index, row in naics.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = index\n",
    "    Naics = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Naics, RDF.type, LAO.Naics))\n",
    "    g.add((Naics, LAO['naicsCode'], Literal(index, datatype=XSD.string)))    \n",
    "    g.add((Naics, LAO['naicsDescription'], Literal(row['Title'], datatype=XSD.string)))    \n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('naics.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "activeBusinesses = pd.read_csv(activeBusinessesData, sep=',', index_col='LOCATION ACCOUNT #')\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 10.1 s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "activeBusinesses[\"FILTERED ZIP\"] = activeBusinesses[\"ZIP CODE\"].str.split(\"-\", expand=True).get(0)\n",
    "activeBusinesses[\"NAICS\"] = activeBusinesses[\"NAICS\"].astype(\"Int64\")\n",
    "#iterate over the league dataframe\n",
    "for index, row in activeBusinesses.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = str(index)\n",
    "    Business = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "\n",
    "    #TYPE\n",
    "    g.add((Business, RDF.type, LAO.Business))\n",
    "    \n",
    "    #DATA PROPERTIES\n",
    "    g.add((Business, LAO['businessId'], Literal(str(index), datatype=XSD.string)))    \n",
    "    g.add((Business, LAO['businessName'], Literal(row['BUSINESS NAME'], datatype=XSD.string)))    \n",
    "    g.add((Business, LAO['doingBusinessName'], Literal(row['DBA NAME'], datatype=XSD.string)))\n",
    "\n",
    "    #OBJECT PROPERTIES\n",
    "    if (row[\"LOCATION START DATE\"] != ''):\n",
    "        g.add((Business, LAO['openedOnDate'], LAO[row['LOCATION START DATE']]))\n",
    "    if (row[\"FILTERED ZIP\"] != ''):\n",
    "        g.add((Business, LAO['locatedInCity'], LAO[urllib.parse.quote(row[\"FILTERED ZIP\"])]))\n",
    "    if (pd.isna(row[\"NAICS\"])):\n",
    "        g.add((Business, LAO['hasNaics'], LAO[row['NAICS']]))\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('activeBusinesses.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clubsUrl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mirco\\Desktop\\Database 2\\bitsei-db2-unipd\\serializeLAdata.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mirco/Desktop/Database%202/bitsei-db2-unipd/serializeLAdata.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the CSV files in memory\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mirco/Desktop/Database%202/bitsei-db2-unipd/serializeLAdata.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m clubs \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(clubsUrl, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, index_col\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mclub_id\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mirco/Desktop/Database%202/bitsei-db2-unipd/serializeLAdata.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#create the graph\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mirco/Desktop/Database%202/bitsei-db2-unipd/serializeLAdata.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m g \u001b[39m=\u001b[39m Graph()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clubsUrl' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "clubs = pd.read_csv(clubsUrl, sep=',', index_col='club_id')\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96.4 ms, sys: 3.49 ms, total: 99.9 ms\n",
      "Wall time: 102 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the club dataframe\n",
    "for index, row in clubs.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the club id as URI\n",
    "    idU = \"club\"+str(index)\n",
    "    Club = URIRef(SO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Club, RDF.type, SO.Club))\n",
    "    g.add((Club, SO['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "    idL = \"league\"+str(row['league_id'])\n",
    "    g.add((Club, SO['competeIn'], URIRef(SO[idL])))\n",
    "    \n",
    "    try:\n",
    "        # get the nationality of the club\n",
    "        nationality = leagues.loc[row['league_id'], 'nationality' ]\n",
    "        # create the RDF node\n",
    "        Country = URIRef(CNS[row['nationality']])\n",
    "        # add the edge connecting the Movie and the Country \n",
    "        g.add((Club, SO['nationality'], Country))    \n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 74 ms, sys: 3.26 ms, total: 77.3 ms\n",
      "Wall time: 78.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'clubs.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "playersFifa = pd.read_csv(playersFifaUrl, sep=',', index_col='sofifa_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='Name', keep_default_na=False, na_values=['_'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the players dataframe\n",
    "for index, row in players.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the player id as URI\n",
    "    idU = \"player\"+str(index)\n",
    "    Player = URIRef(SO[idU])\n",
    "    # the transferMarkt profile has as URI, the URL of the profile in the website\n",
    "    TransfermarktProfile = URIRef(row['url'])\n",
    "    \n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Player, RDF.type, SO.Player))\n",
    "    g.add((TransfermarktProfile, RDF.type, SO.TransfermarktProfile))\n",
    "    g.add((TransfermarktProfile, SO['isAbout'], Player))\n",
    "    \n",
    "    #process player name\n",
    "    name = row['name'].split('-')\n",
    "\n",
    "    if (len(name)>1):\n",
    "        g.add((Player, SO['firstName'], Literal(name[0], datatype=XSD.string)))\n",
    "        g.add((Player, SO['lastName'], Literal(name[1], datatype=XSD.string)))\n",
    "    else:\n",
    "        g.add((Player, SO['lastName'], Literal(name[0], datatype=XSD.string)))\n",
    "        \n",
    "    #there can be more than one position per player\n",
    "    for pos in row['position'].split(' - '):\n",
    "        g.add((Player, SO['position'], Literal(pos.lower(), datatype=XSD.string)))\n",
    "    \n",
    "    if not(row['club_id']==''):\n",
    "        idC = \"club\"+str(row['club_id'])\n",
    "        g.add((Player, SO['playFor'], URIRef(SO[idC])))\n",
    "\n",
    "#iterate over the fifa dataframe\n",
    "for index, row in playersFifa.iterrows():\n",
    "    pname = row['short_name'].lower()\n",
    "    if ('.' in pname):\n",
    "        # get last name\n",
    "        # in the fifa dataset we have short names as L. Messi so we delete the L. \n",
    "        # we need to check if the last name contains a space\n",
    "        pname = row['short_name'].split('.')[1].lower().strip()\n",
    "        if ' ' in pname:\n",
    "            i = 0\n",
    "            for t in pname.split(' '):\n",
    "                if i == 0:\n",
    "                    pname = t.lower()\n",
    "                else:\n",
    "                    pname = pname + \"-\" + t.lower()\n",
    "                i += 1           \n",
    "    elif(' ' in pname):\n",
    "        # here we have to handle Cristiano Ronaldo mapping it to cristiano-ronaldo to maximize the match in the players dataframe \n",
    "        i = 0\n",
    "        for t in row['short_name'].split(' '):\n",
    "            if i == 0:\n",
    "                pname = t.lower()\n",
    "            else:\n",
    "                pname = pname + \"-\" + t.lower()\n",
    "            i += 1\n",
    "    pname = strip_accents(pname)\n",
    "    \n",
    "    # find sim with the full name \n",
    "    fullname = row['long_name'].lower()\n",
    "    i = 0\n",
    "    for t in fullname.split(' '):\n",
    "        if i == 0:\n",
    "            fullname = t.lower()\n",
    "        else:\n",
    "            fullname = fullname + \"-\" + t.lower()\n",
    "        i += 1 \n",
    "    fullname = strip_accents(fullname)\n",
    "    # check the players with that last name\n",
    "    names =  players[players['name'].str.contains(pname)]['name']\n",
    "    #find max similarity    \n",
    "    maxN = 0\n",
    "    playerId = ''\n",
    "    for n in names:\n",
    "        sim = SequenceMatcher(None, fullname, n).ratio()\n",
    "        if (maxN < sim):\n",
    "            maxN = sim\n",
    "            playerId = players.loc[players['name'] == n].index[0]\n",
    "        \n",
    "    #if we get a valid playerId we can connect the Fifa stats to the transfermrkt player\n",
    "    if (playerId != ''):\n",
    "        #remove the row from the player dataframe to avoid futher matchings (we know data will contain errors)\n",
    "        players = players.drop(index=playerId)\n",
    "        idU = \"player\"+str(playerId)\n",
    "        Player = URIRef(SO[idU])\n",
    "        g.add((Player, SO['overallFifaValue'], Literal(row['overall'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['growthFifaPotential'], Literal(row['potential'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['economicValue'], Literal(row['value_eur'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['annualWage'], Literal(row['wage_eur'], datatype=XSD.int))) \n",
    "        \n",
    "        pFeatures = str(row['player_tags'])\n",
    "        if pFeatures != '_' and pFeatures != '':\n",
    "            pFeatures = pFeatures.split(',')\n",
    "            for feature in pFeatures:\n",
    "                feature = feature.strip()\n",
    "                feature = re.sub('#', '', feature)\n",
    "                g.add((Player, SO['playerFeature'], Literal(feature, datatype=XSD.string)))\n",
    "        \n",
    "        if row['contract_valid_until'] != '_' and row['contract_valid_until'] != '':\n",
    "            g.add((Player, SO['contractValidTo'], Literal(int(row['contract_valid_until']), datatype=XSD.gYear)))        \n",
    "\n",
    "        g.add((Player, SO['birthday'], Literal(row['dob'], datatype=XSD.date)))\n",
    "        g.add((Player, SO['height'], Literal(row['height_cm'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['weight'], Literal(row['weight_kg'], datatype=XSD.int)))\n",
    "        \n",
    "        \n",
    "        nationality = row['nationality'] \n",
    "        nationality = nationality.replace(\" \", \"_\")\n",
    "        # create the RDF node\n",
    "        Country = URIRef(CNS[nationality])\n",
    "        # add the edge connecting the Movie and the Country \n",
    "        g.add((Player, SO['nationality'], Country))   \n",
    "\n",
    "        # Homework: extend the code to populate the 'propertyOf' edge\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'players.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "apps = pd.read_csv(appearancesUrl, sep=',', index_col='appearance_id', keep_default_na=False, na_values=['_'])\n",
    "games = pd.read_csv(gamesUrl, sep=',', index_col='game_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over the games dataframe\n",
    "for index, row in games.iterrows():\n",
    "    # we use the transfermrket URL as URI\n",
    "    Game = URIRef(row['url'])\n",
    "    g.add((Game, RDF.type, SO.Game))\n",
    "    idU1 = \"club\"+str(row['home_club_id'])\n",
    "    idU2 = \"club\"+str(row['away_club_id'])\n",
    "    HomeClub = URIRef(SO[idU1])\n",
    "    AwayClub = URIRef(SO[idU2])\n",
    "    g.add((Game, SO['homeClub'], HomeClub))\n",
    "    g.add((Game, SO['awayClub'], AwayClub))    \n",
    "    g.add((Game, SO['matchDay'], Literal(row['date'], datatype=XSD.date)))\n",
    "    g.add((Game, SO['homeClubGoals'], Literal(row['home_club_goals'], datatype=XSD.int)))\n",
    "    g.add((Game, SO['awayClubGoals'], Literal(row['away_club_goals'], datatype=XSD.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 4.7 s, sys: 55.8 ms, total: 4.75 s\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'games.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the full players dataframe\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldgameid = ''\n",
    "for index, row in apps.iterrows():\n",
    "    idA = \"appearance\"+str(index)\n",
    "    idP = \"player\"+str(row['player_id'])\n",
    "    idG = \"game\"+currgameid\n",
    "    Appearance = URIRef(SO[idA])\n",
    "    Player = URIRef(SO[idP])\n",
    "    currgameid = str(row['game_id'])\n",
    "    Game = URIRef(SO[idG])\n",
    "    g.add((Appearance, RDF.type, SO.Appearance))\n",
    "    g.add((Player, SO['appearIn'], Appearance))\n",
    "    g.add((Appearance, SO['playIn'], Game))\n",
    "\n",
    "    g.add((Appearance, SO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['minutesPlayed'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['yellowCard'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['redCard'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "\n",
    "    #add this triple only once per game\n",
    "    if (currgameid != oldgameid):\n",
    "        idL = \"league\"+str(row['league_id'])\n",
    "        g.add((Game, SO['belongTo'], URIRef(SO[idL])))\n",
    "        oldgameid = currgameid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 1min 22s, sys: 1.32 s, total: 1min 23s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'appearances.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
