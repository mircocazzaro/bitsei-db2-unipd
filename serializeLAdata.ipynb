{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the Loa Angeles Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "# CHECK DATE \n",
    "from datetime import datetime, date, timedelta\n",
    "import urllib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "activeBusinessesData = 'datasets/ACTIVE BUSINESSES/Listing_of_Active_Businesses_parsed.csv'\n",
    "closedBusinessesData = 'datasets/CLOSED BUSINESSES/All_Closed_Businesses_20231101.csv'\n",
    "laCovidData = 'datasets/COVID DATA/sorted_los_angeles_covid_data.csv'\n",
    "crimeData1 = 'datasets/CRIME DATA/Crime_Data_from_2020_to_Present_1.csv'\n",
    "crimeData2 = 'datasets/CRIME DATA/Crime_Data_from_2020_to_Present_2.csv'\n",
    "crimeData3 = 'datasets/CRIME DATA/Crime_Data_from_2020_to_Present_3.csv'\n",
    "crimeCodesDescData = 'datasets/CRIME DATA/CrimesCodesAndDesc_listed.csv'\n",
    "moCodesData = 'datasets/CRIME DATA/CrimesCodesAndDesc_listed.csv'\n",
    "naicsData = 'datasets/CLOSED BUSINESSES/2022_NAICS_Descriptions.csv'\n",
    "weaponData = 'datasets/CRIME DATA/weapon_ds.csv'\n",
    "\n",
    "\n",
    "# saving folder\n",
    "savePath =  path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "#CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "LAO = Namespace(\"http://www.bitsei.it/losAngelesOntology/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Last_Update</th>\n",
       "      <th>Active</th>\n",
       "      <th>Deaths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solodata</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26</th>\n",
       "      <td>2023-12-26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <td>2023-12-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2193 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Last_Update  Active  Deaths\n",
       "solodata                              \n",
       "2018-01-01  2018-01-01       0       0\n",
       "2018-01-02  2018-01-02       0       0\n",
       "2018-01-03  2018-01-03       0       0\n",
       "2018-01-04  2018-01-04       0       0\n",
       "2018-01-05  2018-01-05       0       0\n",
       "...                ...     ...     ...\n",
       "2023-12-26  2023-12-26       0       0\n",
       "2023-12-27  2023-12-27       0       0\n",
       "2023-12-28  2023-12-28       0       0\n",
       "2023-12-29  2023-12-29       0       0\n",
       "2023-12-30  2023-12-30       0       0\n",
       "\n",
       "[2193 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "laCovid = pd.read_csv(laCovidData, sep=',') #, dtype={'Active':int, 'Deaths':int}\n",
    "\n",
    "laCovid[\"Last_Update\"] = pd.to_datetime(laCovid['Last_Update'])\n",
    "laCovid['solodata'] = laCovid['Last_Update'].dt.date\n",
    "laCovid['Active'] = laCovid['Active'].astype('Int64')\n",
    "laCovid['Deaths'] = laCovid['Deaths'].astype('Int64')\n",
    "\n",
    "laCovid = laCovid[['Last_Update', 'solodata', 'Active', 'Deaths']]\n",
    "\n",
    "start_date = pd.to_datetime('2018-01-01')\n",
    "end_date = pd.to_datetime('2023-12-31')\n",
    "for i in daterange(start_date, end_date):\n",
    "    if i.date() not in laCovid['solodata'].values:\n",
    "        new_row = {\n",
    "            'solodata': i.date(),\n",
    "            'Last_Update' : i, #pd.to_datetime(str(i), '%Y-%m-%d %H%M%S')\n",
    "            'Active': 0,\n",
    "            'Deaths': 0\n",
    "        }\n",
    "        laCovid = pd.concat([laCovid, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "laCovid.sort_values(by='solodata', inplace=True)\n",
    "laCovid.set_index(\"solodata\", inplace=True)\n",
    "laCovid.to_csv(\"coviddc.csv\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)\n",
    "\n",
    "laCovid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 195 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:11: UserWarning: Code: datetime is not defined in namespace XSD\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the league dataframe\n",
    "for index, row in laCovid.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = str(index)\n",
    "    Day = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Day, RDF.type, LAO.Day))\n",
    "    g.add((Day, LAO['hasDate'], Literal(str(row['Last_Update']), datatype=XSD.datetime)))    \n",
    "    g.add((Day, LAO['hasActiveCases'], Literal(row['Active'], datatype=XSD.int)))    \n",
    "    g.add((Day, LAO['hasNOfDeaths'], Literal(row['Deaths'], datatype=XSD.int)))    \n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('covidDays.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    CITY\n",
      "ZIP CODE                \n",
      "91344      GRANADA HILLS\n",
      "90015        LOS ANGELES\n",
      "91306           WINNETKA\n",
      "90020        LOS ANGELES\n",
      "91303        CANOGA PARK\n",
      "...                  ...\n",
      "90292     MARINA DEL REY\n",
      "90710        HARBOR CITY\n",
      "90247            GARDENA\n",
      "91436             ENCINO\n",
      "91411           VAN NUYS\n",
      "\n",
      "[100 rows x 1 columns]\n",
      "763\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "cities1 = pd.read_csv(closedBusinessesData, sep=',', index_col='LOCATION ACCOUNT #')\n",
    "cities1 = cities1[['CITY','ZIP CODE']]\n",
    "\n",
    "\n",
    "cities2 = pd.read_csv(activeBusinessesData, sep=',', index_col='LOCATION ACCOUNT #')\n",
    "cities2 = cities2[['CITY','ZIP CODE']]\n",
    "\n",
    "\n",
    "cities = pd.merge(cities1, cities2)\n",
    "cities[\"ZIP CODE\"] = cities[\"ZIP CODE\"].str.split(\"-\", expand=True).get(0)\n",
    "cities = cities[cities['ZIP CODE'] != '']\n",
    "cities = cities.drop_duplicates()\n",
    "cities.set_index(\"ZIP CODE\", inplace=True)\n",
    "\n",
    "print(cities.head(100))\n",
    "print(len(cities))\n",
    "\n",
    "cities.to_csv('cities.csv', index=True)\n",
    "\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 94.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the league dataframe\n",
    "for index, row in cities.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = str(index)\n",
    "    City = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((City, RDF.type, LAO.City))\n",
    "    g.add((City, LAO['cityZipCode'], Literal(str(index), datatype=XSD.string)))    \n",
    "    g.add((City, LAO['cityName'], Literal(row['CITY'], datatype=XSD.string)))    \n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('cities.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Title\n",
      "Code                                              \n",
      "11      Agriculture, Forestry, Fishing and Hunting\n",
      "111                                Crop Production\n",
      "1111                     Oilseed and Grain Farming\n",
      "11111                              Soybean Farming\n",
      "111110                             Soybean Farming\n",
      "11112             Oilseed (except Soybean) Farming\n",
      "111120            Oilseed (except Soybean) Farming\n",
      "11113                     Dry Pea and Bean Farming\n",
      "111130                    Dry Pea and Bean Farming\n",
      "11114                                Wheat Farming\n",
      "111140                               Wheat Farming\n",
      "11115                                 Corn Farming\n",
      "111150                                Corn Farming\n",
      "11116                                 Rice Farming\n",
      "111160                                Rice Farming\n",
      "11119                          Other Grain Farming\n",
      "111191       Oilseed and Grain Combination Farming\n",
      "111199                     All Other Grain Farming\n",
      "1112                   Vegetable and Melon Farming\n",
      "11121                  Vegetable and Melon Farming\n",
      "2125\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "naics = pd.read_csv(naicsData, sep=',', index_col='Code')\n",
    "naics = naics[['Title']]\n",
    "naics[\"Title\"] = naics[\"Title\"].replace(\"T$\", \"\", regex=True)\n",
    "print(naics.head(20))\n",
    "print(len(naics))\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 259 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the league dataframe\n",
    "for index, row in naics.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = index\n",
    "    Naics = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Naics, RDF.type, LAO.Naics))\n",
    "    g.add((Naics, LAO['naicsCode'], Literal(index, datatype=XSD.string)))    \n",
    "    g.add((Naics, LAO['naicsDescription'], Literal(row['Title'], datatype=XSD.string)))    \n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('naics.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "activeBusinesses = pd.read_csv(activeBusinessesData, sep=',', index_col='LOCATION ACCOUNT #')\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 10.1 s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "activeBusinesses[\"FILTERED ZIP\"] = activeBusinesses[\"ZIP CODE\"].str.split(\"-\", expand=True).get(0)\n",
    "activeBusinesses[\"NAICS\"] = activeBusinesses[\"NAICS\"].astype(\"Int64\")\n",
    "#iterate over the league dataframe\n",
    "for index, row in activeBusinesses.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = str(index)\n",
    "    Business = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "\n",
    "    #TYPE\n",
    "    g.add((Business, RDF.type, LAO.Business))\n",
    "    \n",
    "    #DATA PROPERTIES\n",
    "    g.add((Business, LAO['businessId'], Literal(str(index), datatype=XSD.string)))    \n",
    "    g.add((Business, LAO['businessName'], Literal(row['BUSINESS NAME'], datatype=XSD.string)))    \n",
    "    g.add((Business, LAO['doingBusinessName'], Literal(row['DBA NAME'], datatype=XSD.string)))\n",
    "\n",
    "    #OBJECT PROPERTIES\n",
    "    if (row[\"LOCATION START DATE\"] != ''):\n",
    "        g.add((Business, LAO['openedOnDate'], LAO[row['LOCATION START DATE']]))\n",
    "    if (row[\"FILTERED ZIP\"] != ''):\n",
    "        g.add((Business, LAO['locatedInCity'], LAO[urllib.parse.quote(row[\"FILTERED ZIP\"])]))\n",
    "    if (pd.isna(row[\"NAICS\"])):\n",
    "        g.add((Business, LAO['hasNaics'], LAO[row['NAICS']]))\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('activeBusinesses.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "areasList = ['Harbor Gateway-HAR', 'Palms - Mar Vista - Del Rey-PLM', 'Bel Air - Beverly Crest-BAR', 'Granada Hills - Knollwood-GHL', 'Mission Hills - Panorama City - North Hills-MSS', 'Encino - Tarzana-ENC', 'Brentwood - Pacific Palisades-BTW', 'West Adams - Baldwin Hills - Leimert-WAD', 'North Hollywood - Valley Village-NHL', 'Arleta - Pacoima-ARL', 'Northeast Los Angeles-NLA', 'Venice-VEN', 'Silver Lake - Echo Park - Elysian Valley-SLK', 'San Pedro-SPD', 'Reseda - West Van Nuys-RES', 'Sun Valley - La Tuna Canyon-SVY', 'Sunland - Tujunga - Lake View Terrace - Shadow Hills - East La Tuna Canyon-SLD', 'Westwood-WWD', 'West Los Angeles-WLA', 'Hollywood-HWD', 'Canoga Park - Winnetka - Woodland Hills - West Hills-CPK', 'Central City North-CCN', 'Chatsworth - Porter Ranch-CHT', 'Wilmington - Harbor City-WLM', 'Sylmar-SYL', 'Wilshire-WIL', 'Central City-CCY', 'Westlake-WLK', 'Port of Los Angeles-PTL', 'Northridge-NRD', 'Van Nuys - North Sherman Oaks-VNY', 'Boyle Heights-BHT', 'Sherman Oaks - Studio City - Toluca Lake - Cahuenga Pass-SHR', 'Westchester - Playa del Rey-WCH', 'Los Angeles International Airport-LAX', 'South Los Angeles-SLA', 'Southeast Los Angeles-SEL']\n",
    "# Split each string in the list into two parts, before and after the dash.\n",
    "city_names = [string.split('-')[0] for string in areasList]\n",
    "acronyms = [string.split('-')[-1] for string in areasList]\n",
    "\n",
    "# Create a DataFrame with the two columns.\n",
    "areas = pd.DataFrame({'CITY NAME': city_names, 'ACRONYM': acronyms})\n",
    "\n",
    "areas.set_index(\"ACRONYM\", inplace=True)\n",
    "\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"lao\", LAO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 31.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the league dataframe\n",
    "for index, row in areas.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the league id as URI\n",
    "    idU = str(index)\n",
    "    Area = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "\n",
    "    #TYPE\n",
    "    g.add((Area, RDF.type, LAO.Area))\n",
    "    \n",
    "    #DATA PROPERTIES\n",
    "    g.add((Area, LAO['areaAcronym'], Literal(str(index), datatype=XSD.string)))    \n",
    "    g.add((Area, LAO['areaName'], Literal(row['CITY NAME'], datatype=XSD.string)))    \n",
    "\n",
    "    #OBJECT PROPERTIES\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    # create the RDF node\n",
    "    # Country = URIRef(CNS[row['nationality']])\n",
    "    # add the edge connecting the Movie and the Country \n",
    "    #g.add((League, SO['nationality'], Country))    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('areas.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STREET ADDRESS</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHIAVE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33.7901+-118.2804</th>\n",
       "      <td>1330 W PACIFIC COAST HIGHWAY SUITE #E</td>\n",
       "      <td>33.7901</td>\n",
       "      <td>-118.2804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33.9603+-118.4471</th>\n",
       "      <td>327 CULVER BLVD UNIT #4</td>\n",
       "      <td>33.9603</td>\n",
       "      <td>-118.4471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34.1468+-118.423</th>\n",
       "      <td>13317 VENTURA BLVD    #B</td>\n",
       "      <td>34.1468</td>\n",
       "      <td>-118.423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32.6665+-117.1073</th>\n",
       "      <td>319 W 18TH STREET</td>\n",
       "      <td>32.6665</td>\n",
       "      <td>-117.1073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33.8301+-118.328</th>\n",
       "      <td>1740   CRENSHAW BLVD</td>\n",
       "      <td>33.8301</td>\n",
       "      <td>-118.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33.9511+-118.2739</th>\n",
       "      <td>94TH</td>\n",
       "      <td>33.9511</td>\n",
       "      <td>-118.2739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34.2687+-118.3107</th>\n",
       "      <td>NASSAU                       AV</td>\n",
       "      <td>34.2687</td>\n",
       "      <td>-118.3107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34.069+-118.3041</th>\n",
       "      <td>W  3RD                          ST</td>\n",
       "      <td>34.069</td>\n",
       "      <td>-118.3041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34.1536+-118.4181</th>\n",
       "      <td>MILBANK                      ST</td>\n",
       "      <td>34.1536</td>\n",
       "      <td>-118.4181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33.9456+-118.2323</th>\n",
       "      <td>FIGUEROA</td>\n",
       "      <td>33.9456</td>\n",
       "      <td>-118.2323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>345202 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          STREET ADDRESS LATITUDE  LONGITUDE\n",
       "CHIAVE                                                                      \n",
       "33.7901+-118.2804  1330 W PACIFIC COAST HIGHWAY SUITE #E  33.7901  -118.2804\n",
       "33.9603+-118.4471                327 CULVER BLVD UNIT #4  33.9603  -118.4471\n",
       "34.1468+-118.423                13317 VENTURA BLVD    #B  34.1468   -118.423\n",
       "32.6665+-117.1073                      319 W 18TH STREET  32.6665  -117.1073\n",
       "33.8301+-118.328                    1740   CRENSHAW BLVD  33.8301   -118.328\n",
       "...                                                  ...      ...        ...\n",
       "33.9511+-118.2739                                   94TH  33.9511  -118.2739\n",
       "34.2687+-118.3107        NASSAU                       AV  34.2687  -118.3107\n",
       "34.069+-118.3041      W  3RD                          ST   34.069  -118.3041\n",
       "34.1536+-118.4181        MILBANK                      ST  34.1536  -118.4181\n",
       "33.9456+-118.2323                               FIGUEROA  33.9456  -118.2323\n",
       "\n",
       "[345202 rows x 3 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "activeBusinesses = pd.read_csv(activeBusinessesData, sep=',')\n",
    "closedBusinesses = pd.read_csv(closedBusinessesData, sep=',')\n",
    "crimeData1_df = pd.read_csv(crimeData1, sep=',')\n",
    "crimeData2_df = pd.read_csv(crimeData2, sep=',')\n",
    "crimeData3_df = pd.read_csv(crimeData3, sep=',')\n",
    "\n",
    "activeBusinesses = activeBusinesses[[\"STREET ADDRESS\", \"LOCATION\"]]\n",
    "closedBusinesses = closedBusinesses[[\"STREET ADDRESS\", \"LOCATION\"]]\n",
    "crimeData1_df = crimeData1_df[[\"LOCATION\", \"LAT\", \"LON\"]]\n",
    "crimeData1_df.rename(columns={\"LOCATION\": \"STREET ADDRESS\", \"LAT\": \"LATITUDE\", \"LON\": \"LONGITUDE\"}, inplace=True)\n",
    "crimeData2_df = crimeData2_df[[\"LOCATION\", \"LAT\", \"LON\"]]\n",
    "crimeData2_df.rename(columns={\"LOCATION\": \"STREET ADDRESS\", \"LAT\": \"LATITUDE\", \"LON\": \"LONGITUDE\"}, inplace=True)\n",
    "crimeData3_df = crimeData3_df[[\"LOCATION\", \"LAT\", \"LON\"]]\n",
    "crimeData3_df.rename(columns={\"LOCATION\": \"STREET ADDRESS\", \"LAT\": \"LATITUDE\", \"LON\": \"LONGITUDE\"}, inplace=True)\n",
    "\n",
    "\n",
    "locations = pd.concat([activeBusinesses, closedBusinesses]).drop_duplicates()\n",
    "locations['LOCATION'] = locations['LOCATION'].str.replace('(','') \n",
    "locations['LOCATION'] = locations['LOCATION'].str.replace(')','')\n",
    "locations['LOCATION'] = locations['LOCATION'].str.replace(' ','')\n",
    "locations[['LATITUDE', 'LONGITUDE']] = locations['LOCATION'].str.split(',', expand=True)\n",
    "locations.drop(columns=['LOCATION'], inplace=True)\n",
    "# Remove the original coordinates column\n",
    "#locations.drop('LOCATION', axis=1, inplace=True)\n",
    "\n",
    "locations = pd.concat([locations, crimeData1_df, crimeData2_df, crimeData3_df]).drop_duplicates()\n",
    "locations['CHIAVE'] = locations['LATITUDE'].astype(\"string\") + '+' + locations['LONGITUDE'].astype(\"string\")\n",
    "locations.drop(locations[locations['CHIAVE'] == '0.0+0.0'].index, inplace = True)\n",
    "locations.set_index('CHIAVE', inplace=True)\n",
    "locations = locations.dropna()\n",
    "\n",
    "\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"lao\", LAO)\n",
    "\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 15.7 s\n",
      "Wall time: 55.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the club dataframe\n",
    "for index, row in locations.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the club id as URI\n",
    "    idU = \"loc\"+str(index)\n",
    "    Location = URIRef(LAO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Location, RDF.type, LAO.Location))\n",
    "    g.add((Location, LAO['hasLatitude'], Literal(str(row['LATITUDE']), datatype=XSD.double)))\n",
    "    g.add((Location, LAO['hasLongitude'], Literal(str(row['LONGITUDE']), datatype=XSD.double)))\n",
    "    g.add((Location, LAO['hasAddress'], Literal(str(row['STREET ADDRESS']), datatype=XSD.string)))\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open('locations.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "playersFifa = pd.read_csv(playersFifaUrl, sep=',', index_col='sofifa_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='Name', keep_default_na=False, na_values=['_'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the players dataframe\n",
    "for index, row in players.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the player id as URI\n",
    "    idU = \"player\"+str(index)\n",
    "    Player = URIRef(SO[idU])\n",
    "    # the transferMarkt profile has as URI, the URL of the profile in the website\n",
    "    TransfermarktProfile = URIRef(row['url'])\n",
    "    \n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Player, RDF.type, SO.Player))\n",
    "    g.add((TransfermarktProfile, RDF.type, SO.TransfermarktProfile))\n",
    "    g.add((TransfermarktProfile, SO['isAbout'], Player))\n",
    "    \n",
    "    #process player name\n",
    "    name = row['name'].split('-')\n",
    "\n",
    "    if (len(name)>1):\n",
    "        g.add((Player, SO['firstName'], Literal(name[0], datatype=XSD.string)))\n",
    "        g.add((Player, SO['lastName'], Literal(name[1], datatype=XSD.string)))\n",
    "    else:\n",
    "        g.add((Player, SO['lastName'], Literal(name[0], datatype=XSD.string)))\n",
    "        \n",
    "    #there can be more than one position per player\n",
    "    for pos in row['position'].split(' - '):\n",
    "        g.add((Player, SO['position'], Literal(pos.lower(), datatype=XSD.string)))\n",
    "    \n",
    "    if not(row['club_id']==''):\n",
    "        idC = \"club\"+str(row['club_id'])\n",
    "        g.add((Player, SO['playFor'], URIRef(SO[idC])))\n",
    "\n",
    "#iterate over the fifa dataframe\n",
    "for index, row in playersFifa.iterrows():\n",
    "    pname = row['short_name'].lower()\n",
    "    if ('.' in pname):\n",
    "        # get last name\n",
    "        # in the fifa dataset we have short names as L. Messi so we delete the L. \n",
    "        # we need to check if the last name contains a space\n",
    "        pname = row['short_name'].split('.')[1].lower().strip()\n",
    "        if ' ' in pname:\n",
    "            i = 0\n",
    "            for t in pname.split(' '):\n",
    "                if i == 0:\n",
    "                    pname = t.lower()\n",
    "                else:\n",
    "                    pname = pname + \"-\" + t.lower()\n",
    "                i += 1           \n",
    "    elif(' ' in pname):\n",
    "        # here we have to handle Cristiano Ronaldo mapping it to cristiano-ronaldo to maximize the match in the players dataframe \n",
    "        i = 0\n",
    "        for t in row['short_name'].split(' '):\n",
    "            if i == 0:\n",
    "                pname = t.lower()\n",
    "            else:\n",
    "                pname = pname + \"-\" + t.lower()\n",
    "            i += 1\n",
    "    pname = strip_accents(pname)\n",
    "    \n",
    "    # find sim with the full name \n",
    "    fullname = row['long_name'].lower()\n",
    "    i = 0\n",
    "    for t in fullname.split(' '):\n",
    "        if i == 0:\n",
    "            fullname = t.lower()\n",
    "        else:\n",
    "            fullname = fullname + \"-\" + t.lower()\n",
    "        i += 1 \n",
    "    fullname = strip_accents(fullname)\n",
    "    # check the players with that last name\n",
    "    names =  players[players['name'].str.contains(pname)]['name']\n",
    "    #find max similarity    \n",
    "    maxN = 0\n",
    "    playerId = ''\n",
    "    for n in names:\n",
    "        sim = SequenceMatcher(None, fullname, n).ratio()\n",
    "        if (maxN < sim):\n",
    "            maxN = sim\n",
    "            playerId = players.loc[players['name'] == n].index[0]\n",
    "        \n",
    "    #if we get a valid playerId we can connect the Fifa stats to the transfermrkt player\n",
    "    if (playerId != ''):\n",
    "        #remove the row from the player dataframe to avoid futher matchings (we know data will contain errors)\n",
    "        players = players.drop(index=playerId)\n",
    "        idU = \"player\"+str(playerId)\n",
    "        Player = URIRef(SO[idU])\n",
    "        g.add((Player, SO['overallFifaValue'], Literal(row['overall'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['growthFifaPotential'], Literal(row['potential'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['economicValue'], Literal(row['value_eur'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['annualWage'], Literal(row['wage_eur'], datatype=XSD.int))) \n",
    "        \n",
    "        pFeatures = str(row['player_tags'])\n",
    "        if pFeatures != '_' and pFeatures != '':\n",
    "            pFeatures = pFeatures.split(',')\n",
    "            for feature in pFeatures:\n",
    "                feature = feature.strip()\n",
    "                feature = re.sub('#', '', feature)\n",
    "                g.add((Player, SO['playerFeature'], Literal(feature, datatype=XSD.string)))\n",
    "        \n",
    "        if row['contract_valid_until'] != '_' and row['contract_valid_until'] != '':\n",
    "            g.add((Player, SO['contractValidTo'], Literal(int(row['contract_valid_until']), datatype=XSD.gYear)))        \n",
    "\n",
    "        g.add((Player, SO['birthday'], Literal(row['dob'], datatype=XSD.date)))\n",
    "        g.add((Player, SO['height'], Literal(row['height_cm'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['weight'], Literal(row['weight_kg'], datatype=XSD.int)))\n",
    "        \n",
    "        \n",
    "        nationality = row['nationality'] \n",
    "        nationality = nationality.replace(\" \", \"_\")\n",
    "        # create the RDF node\n",
    "        Country = URIRef(CNS[nationality])\n",
    "        # add the edge connecting the Movie and the Country \n",
    "        g.add((Player, SO['nationality'], Country))   \n",
    "\n",
    "        # Homework: extend the code to populate the 'propertyOf' edge\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'players.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "apps = pd.read_csv(appearancesUrl, sep=',', index_col='appearance_id', keep_default_na=False, na_values=['_'])\n",
    "games = pd.read_csv(gamesUrl, sep=',', index_col='game_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over the games dataframe\n",
    "for index, row in games.iterrows():\n",
    "    # we use the transfermrket URL as URI\n",
    "    Game = URIRef(row['url'])\n",
    "    g.add((Game, RDF.type, SO.Game))\n",
    "    idU1 = \"club\"+str(row['home_club_id'])\n",
    "    idU2 = \"club\"+str(row['away_club_id'])\n",
    "    HomeClub = URIRef(SO[idU1])\n",
    "    AwayClub = URIRef(SO[idU2])\n",
    "    g.add((Game, SO['homeClub'], HomeClub))\n",
    "    g.add((Game, SO['awayClub'], AwayClub))    \n",
    "    g.add((Game, SO['matchDay'], Literal(row['date'], datatype=XSD.date)))\n",
    "    g.add((Game, SO['homeClubGoals'], Literal(row['home_club_goals'], datatype=XSD.int)))\n",
    "    g.add((Game, SO['awayClubGoals'], Literal(row['away_club_goals'], datatype=XSD.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 4.7 s, sys: 55.8 ms, total: 4.75 s\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'games.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the full players dataframe\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldgameid = ''\n",
    "for index, row in apps.iterrows():\n",
    "    idA = \"appearance\"+str(index)\n",
    "    idP = \"player\"+str(row['player_id'])\n",
    "    idG = \"game\"+currgameid\n",
    "    Appearance = URIRef(SO[idA])\n",
    "    Player = URIRef(SO[idP])\n",
    "    currgameid = str(row['game_id'])\n",
    "    Game = URIRef(SO[idG])\n",
    "    g.add((Appearance, RDF.type, SO.Appearance))\n",
    "    g.add((Player, SO['appearIn'], Appearance))\n",
    "    g.add((Appearance, SO['playIn'], Game))\n",
    "\n",
    "    g.add((Appearance, SO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['minutesPlayed'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['yellowCard'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['redCard'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "\n",
    "    #add this triple only once per game\n",
    "    if (currgameid != oldgameid):\n",
    "        idL = \"league\"+str(row['league_id'])\n",
    "        g.add((Game, SO['belongTo'], URIRef(SO[idL])))\n",
    "        oldgameid = currgameid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: user 1min 22s, sys: 1.32 s, total: 1min 23s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'appearances.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
