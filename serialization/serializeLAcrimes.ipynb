{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the LosAngelesCovid Ontology with the crimes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "# CHECK DATE \n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "activeBusinessesData = '../datasets/ACTIVE BUSINESSES/Listing_of_Active_Businesses_parsed.csv'\n",
    "closedBusinessesData = '../datasets/CLOSED BUSINESSES/All_Closed_Businesses_20231101.csv'\n",
    "laCovidData = '../datasets/COVID DATA/sorted_los_angeles_covid_data.csv'\n",
    "crimeData1 = '../datasets/CRIME DATA/Crime_Data_from_2020_to_Present_1.csv'\n",
    "crimeData2 = '../datasets/CRIME DATA/Crime_Data_from_2020_to_Present_2.csv'\n",
    "crimeData3 = '../datasets/CRIME DATA/Crime_Data_from_2020_to_Present_3.csv'\n",
    "crimeCodesDescData = '../datasets/CRIME DATA/CrimesCodesAndDesc_listed.csv'\n",
    "moCodesData = '../datasets/CRIME DATA/MO_CODES_Numerical_20191119.csv'\n",
    "weaponData = '../datasets/CRIME DATA/weapon_ds.csv'\n",
    "premisCodesData = '../datasets/CRIME DATA/premisCodesData.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "LAO = Namespace(\"http://www.bitsei.it/losAngelesOntology/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime Victims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "laVictims1 = pd.read_csv(crimeData1, sep=',', usecols=['Vict Age', 'Vict Sex', 'Vict Descent']) \n",
    "laVictims2 = pd.read_csv(crimeData2, sep=',', usecols=['Vict Age', 'Vict Sex', 'Vict Descent'])\n",
    "laVictims3 = pd.read_csv(crimeData3, sep=',', usecols=['Vict Age', 'Vict Sex', 'Vict Descent'])\n",
    "\n",
    "print(laVictims1.head())\n",
    "print(laVictims2.head())\n",
    "print(laVictims3.head())\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "# Create a set to store unique combinations of sex, age, and descent\n",
    "unique_entries = set()\n",
    "\n",
    "#iterate over the laVictims1 dataframe\n",
    "for index, row in laVictims1.iterrows():\n",
    "    \n",
    "    # Extract information from the current row\n",
    "    victim_age = int(row['Vict Age'])\n",
    "    if(victim_age < 0):\n",
    "        victim_age = 0\n",
    "    \n",
    "    victim_sex = str(row['Vict Sex'])\n",
    "    if(victim_sex == 'nan' or victim_sex == '-'):\n",
    "        victim_sex = 'X'\n",
    "\n",
    "    victim_descent = str(row['Vict Descent'])\n",
    "    if(victim_descent == 'nan' or victim_descent == '-'):\n",
    "        victim_descent = 'X'\n",
    "    \n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the victim id as URI\n",
    "    idU = \"victim-\"+str(victim_sex)+'-'+str(victim_age)+'-'+str(victim_descent)\n",
    "    Victim = URIRef(LAO[idU])\n",
    "\n",
    "    # If the current combination doesn't already exist\n",
    "    if ((victim_sex, victim_age, victim_descent) not in unique_entries) and ((victim_age != 0) or (victim_sex != 'X') or (victim_descent != 'X')):\n",
    "        # Add triples using store's add() method.\n",
    "        g.add((Victim, RDF.type, LAO.Victim))\n",
    "        g.add((Victim, LAO['victimSex'], Literal(str(victim_sex), datatype=XSD.string)))\n",
    "        g.add((Victim, LAO['victimAge'], Literal((victim_age), datatype=XSD.int)))    \n",
    "        g.add((Victim, LAO['victimDescent'], Literal(str(victim_descent), datatype=XSD.string)))    \n",
    "\n",
    "        unique_entries.add((victim_sex, victim_age, victim_descent))\n",
    "    \n",
    "    \n",
    "#iterate over the laVictims2 dataframe\n",
    "for index, row in laVictims2.iterrows():\n",
    "    \n",
    "    # Extract information from the current row\n",
    "    victim_age = int(row['Vict Age'])\n",
    "    if(victim_age < 0):\n",
    "        victim_age = 0\n",
    "    \n",
    "    victim_sex = str(row['Vict Sex'])\n",
    "    if(victim_sex == 'nan'):\n",
    "        victim_sex = 'X'\n",
    "\n",
    "    victim_descent = str(row['Vict Descent'])\n",
    "    if(victim_descent == 'nan'):\n",
    "        victim_descent = 'X'\n",
    "    \n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the victim id as URI\n",
    "    idU = \"victim-\"+str(victim_sex)+'-'+str(victim_age)+'-'+str(victim_descent)\n",
    "    Victim = URIRef(LAO[idU])\n",
    "\n",
    "    # If the current combination doesn't already exist\n",
    "    if ((victim_sex, victim_age, victim_descent) not in unique_entries) and ((victim_age != 0) or (victim_sex != 'X') or (victim_descent != 'X')):\n",
    "        # Add triples using store's add() method.\n",
    "        g.add((Victim, RDF.type, LAO.Victim))\n",
    "        g.add((Victim, LAO['victimSex'], Literal(str(victim_sex), datatype=XSD.string)))\n",
    "        g.add((Victim, LAO['victimAge'], Literal((victim_age), datatype=XSD.int)))    \n",
    "        g.add((Victim, LAO['victimDescent'], Literal(str(victim_descent), datatype=XSD.string)))    \n",
    "\n",
    "        unique_entries.add((victim_sex, victim_age, victim_descent))\n",
    "\n",
    "\n",
    "#iterate over the laVictims3 dataframe\n",
    "for index, row in laVictims3.iterrows():\n",
    "    \n",
    "    # Extract information from the current row\n",
    "    victim_age = int(row['Vict Age'])\n",
    "    if(victim_age < 0):\n",
    "        victim_age = 0\n",
    "    \n",
    "    victim_sex = str(row['Vict Sex'])\n",
    "    if(victim_sex == 'nan'):\n",
    "        victim_sex = 'X'\n",
    "\n",
    "    victim_descent = str(row['Vict Descent'])\n",
    "    if(victim_descent == 'nan'):\n",
    "        victim_descent = 'X'\n",
    "    \n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the victim id as URI\n",
    "    idU = \"victim-\"+str(victim_sex)+'-'+str(victim_age)+'-'+str(victim_descent)\n",
    "    Victim = URIRef(LAO[idU])\n",
    "\n",
    "    # If the current combination doesn't already exist\n",
    "    if ((victim_sex, victim_age, victim_descent) not in unique_entries) and ((victim_age != 0) or (victim_sex != 'X') or (victim_descent != 'X')):\n",
    "        # Add triples using store's add() method.\n",
    "        g.add((Victim, RDF.type, LAO.Victim))\n",
    "        g.add((Victim, LAO['victimSex'], Literal(str(victim_sex), datatype=XSD.string)))\n",
    "        g.add((Victim, LAO['victimAge'], Literal((victim_age), datatype=XSD.int)))    \n",
    "        g.add((Victim, LAO['victimDescent'], Literal(str(victim_descent), datatype=XSD.string)))    \n",
    "\n",
    "        unique_entries.add((victim_sex, victim_age, victim_descent))\n",
    "\n",
    "print(\"--- saving serialization ---\")\n",
    "with open('crimeVictims.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modus Operandi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(moCodesData, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "modified_lines = [line.replace(',', '>', 1) for line in lines]\n",
    "\n",
    "with open(moCodesData, 'w') as file:\n",
    "    file.writelines(modified_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "modusOperandi = pd.read_csv(moCodesData, sep='>')\n",
    "print(modusOperandi.head())\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the laVictims1 dataframe\n",
    "for index, row in modusOperandi.iterrows():\n",
    "    \n",
    "    # Extract information from the current row\n",
    "    print(f\"{row['Code']} - {row['Description']}\")\n",
    "\n",
    "    moCode = int(row['Code'])\n",
    "    moDesc = str(row['Description'])\n",
    "    \n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the modusOperandi Code as URI\n",
    "    idU = \"modusOperandi\"+str(moCode)\n",
    "    Mo = URIRef(LAO[idU])\n",
    "\n",
    "    g.add((Mo, RDF.type, LAO.ModusOperandi))\n",
    "    g.add((Mo, LAO['moCode'], Literal((moCode), datatype=XSD.int)))    \n",
    "    g.add((Mo, LAO['moDesc'], Literal(str(moDesc), datatype=XSD.string)))    \n",
    "    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('crimeModusOperandi.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weapons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(weaponData, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "modified_lines = [line.replace(',', '>', 1) for line in lines]\n",
    "\n",
    "with open(weaponData, 'w') as file:\n",
    "    file.writelines(modified_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "weapons = pd.read_csv(weaponData, sep='>')\n",
    "print(weapons.head())\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the laVictims1 dataframe\n",
    "for index, row in weapons.iterrows():\n",
    "    \n",
    "    # Extract information from the current row\n",
    "    print(f\"{row['Weapon Used Cd']} - {row['Weapon Desc']}\")\n",
    "\n",
    "    weaponCode = int(row['Weapon Used Cd'])\n",
    "    weaponDesc = str(row['Weapon Desc'])\n",
    "    \n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the modusOperandi Code as URI\n",
    "    idU = \"weapon\"+str(weaponCode)\n",
    "    Weapon = URIRef(LAO[idU])\n",
    "\n",
    "    g.add((Weapon, RDF.type, LAO.ModusOperandi))\n",
    "    g.add((Weapon, LAO['weaponCode'], Literal((weaponCode), datatype=XSD.int)))    \n",
    "    g.add((Weapon, LAO['weaponDesc'], Literal(str(weaponDesc), datatype=XSD.string)))    \n",
    "    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('crimeWeapons.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premis Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(premisCodesData, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "modified_lines = [line.replace(',', '>', 1) for line in lines]\n",
    "\n",
    "with open(premisCodesData, 'w') as file:\n",
    "    file.writelines(modified_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "premisCodes = pd.read_csv(premisCodesData, sep='>')\n",
    "print(premisCodes.head())\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the laVictims1 dataframe\n",
    "for index, row in premisCodes.iterrows():\n",
    "    \n",
    "    # Extract information from the current row\n",
    "    print(f\"{row['Premis Cd']} - {row['Premis Desc']}\")\n",
    "\n",
    "    premisCode = int(row['Premis Cd'])\n",
    "    premisDesc = str(row['Premis Desc'])\n",
    "    \n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the modusOperandi Code as URI\n",
    "    idU = \"premis\"+str(premisCode)\n",
    "    Premis = URIRef(LAO[idU])\n",
    "\n",
    "    g.add((Premis, RDF.type, LAO.Premis))\n",
    "    g.add((Premis, LAO['premisCode'], Literal((premisCode), datatype=XSD.int)))    \n",
    "    g.add((Premis, LAO['premisDesc'], Literal(str(premisDesc), datatype=XSD.string)))    \n",
    "    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('crimePremis.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(crimeCodesDescData, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "modified_lines = [line.replace(',', '>', 1) for line in lines]\n",
    "\n",
    "with open(crimeCodesDescData, 'w') as file:\n",
    "    file.writelines(modified_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "crimes = pd.read_csv(crimeCodesDescData, sep='>')\n",
    "print(crimes.head())\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the laVictims1 dataframe\n",
    "for index, row in crimes.iterrows():\n",
    "    \n",
    "    # Extract information from the current row\n",
    "    print(f\"{row['Code']} - {row['Description']}\")\n",
    "\n",
    "    crimeCode = int(row['Code'])\n",
    "    crimeDesc = str(row['Description'])\n",
    "    \n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the modusOperandi Code as URI\n",
    "    idU = \"crime\"+str(crimeCode)\n",
    "    Crime = URIRef(LAO[idU])\n",
    "\n",
    "    g.add((Crime, RDF.type, LAO.Crime))\n",
    "    g.add((Crime, LAO['crimeCode'], Literal((crimeCode), datatype=XSD.int)))    \n",
    "    g.add((Crime, LAO['crimeDesc'], Literal(str(crimeDesc), datatype=XSD.string)))    \n",
    "    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('crimeCrimesTypology.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "crimeEvents1 = pd.read_csv(crimeData1, sep=',', index_col='DR_NO')\n",
    "crimeEvents2 = pd.read_csv(crimeData2, sep=',', index_col='DR_NO')\n",
    "crimeEvents3 = pd.read_csv(crimeData3, sep=',', index_col='DR_NO')\n",
    "\n",
    "crimeEvents = pd.concat([crimeEvents1, crimeEvents2, crimeEvents3])\n",
    "crimeEvents['Date Rptd'] = pd.to_datetime(crimeEvents['Date Rptd'], format='%m/%d/%Y').dt.strftime('%Y-%m-%d')\n",
    "crimeEvents['DATE OCC'] = pd.to_datetime(crimeEvents['DATE OCC'], format='%m/%d/%Y').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "print(crimeEvents.head())\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the club dataframe\n",
    "for index, row in crimeEvents.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the club id as URI\n",
    "    idU = \"crimeEvent\"+str(index)\n",
    "    CrimeEvent = URIRef(LAO[idU])\n",
    "    \n",
    "    # Add triples using store's add() method.\n",
    "    \n",
    "    # TYPE\n",
    "    g.add((CrimeEvent, RDF.type, LAO.CrimeEvent))\n",
    "\n",
    "    # DATA PROPERTIES\n",
    "    g.add((CrimeEvent, LAO['crimeId'], Literal(str(index), datatype=XSD.string)))\n",
    "\n",
    "    if(pd.isna(row['TIME OCC']) == False):\n",
    "        time_occ = str(row['TIME OCC'])\n",
    "        if(len(time_occ) < 4):\n",
    "            zeros_needed = 4 - len(time_occ)\n",
    "            time_occ = '0' * zeros_needed + time_occ \n",
    "        # Extract hours and minutes\n",
    "        hours = time_occ[:2]\n",
    "        minutes = time_occ[2:]\n",
    "        if(int(hours) > 23):\n",
    "            print(f'hours format error > 23: {hours}')\n",
    "            hours = 24\n",
    "        if(int(minutes) > 59):\n",
    "            print(f'minutes format error > 59: {minutes}')\n",
    "        store_time = '1970-01-01T' + hours + ':' + minutes + ':00'\n",
    "        #print(f'time_occ: {time_occ} - hours: {hours} - minutes: {minutes} - store_time: {store_time} \\n')\n",
    "        g.add((CrimeEvent, LAO['timeOccurred'], Literal(store_time, datatype=XSD.datetime)))\n",
    "\n",
    "    part = int(row['Part 1-2'])\n",
    "    if (part == 2):\n",
    "        g.add((CrimeEvent, LAO['reportedToFbi'], Literal(True, datatype=XSD.boolean)))\n",
    "    else:    \n",
    "        g.add((CrimeEvent, LAO['reportedToFbi'], Literal(False, datatype=XSD.boolean)))\n",
    "\n",
    "    # OBJECT PROPERTIES\n",
    "    if(pd.isna(row[\"Date Rptd\"]) == False):\n",
    "        g.add((CrimeEvent, LAO['reportedOnDate'], LAO['day' + str(row['Date Rptd'])]))\n",
    "    \n",
    "    if(pd.isna(row[\"DATE OCC\"]) == False):\n",
    "        g.add((CrimeEvent, LAO['occuredOnDate'], LAO['day' + str(row['DATE OCC'])]))\n",
    "    \n",
    "    if(pd.isna(row[\"Crm Cd\"]) == False):\n",
    "        g.add((CrimeEvent, LAO['isOfType'], LAO['crime' + str(row['Crm Cd'])]))\n",
    "    \n",
    "    if(pd.isna(row[\"Mocodes\"]) == False):\n",
    "        mocodes_list = row['Mocodes'].split()\n",
    "        mocodes_integers = [int(code) for code in mocodes_list]\n",
    "        for code in mocodes_integers:\n",
    "            g.add((CrimeEvent, LAO['hasModusOperandi'], LAO['modusOperandi' + str(code)]))\n",
    "    \n",
    "    # Extract information about the victim from the current row\n",
    "    victim_age = int(row['Vict Age'])\n",
    "    if(victim_age < 0 or pd.isna((row['Vict Age']))):\n",
    "        victim_age = 0\n",
    "    \n",
    "    victim_sex = str(row['Vict Sex'])\n",
    "    if(victim_sex == '-' or pd.isna(row['Vict Sex'])):\n",
    "        victim_sex = 'X'\n",
    "\n",
    "    victim_descent = str(row['Vict Descent'])\n",
    "    if(victim_descent == '-' or pd.isna(row['Vict Descent'])):\n",
    "        victim_descent = 'X'\n",
    "    \n",
    "    g.add((CrimeEvent, LAO['occurredOnVictim'], LAO['victim-' + victim_sex + '-' + str(victim_age) + '-' + victim_descent]))\n",
    "    \n",
    "    if(pd.isna(row['Premis Cd']) == False):\n",
    "        premis_code = int(row['Premis Cd'])\n",
    "        g.add((CrimeEvent, LAO['hasPremis'], LAO['premis' + str(premis_code)]))\n",
    "\n",
    "    if(pd.isna(row['Weapon Used Cd']) == False):\n",
    "        weapon_code = int(row['Weapon Used Cd'])\n",
    "        g.add((CrimeEvent, LAO['usedWeapon'], LAO['weapon' + str(weapon_code)]))\n",
    "\n",
    "    if(pd.isna(row['LAT']) == False and pd.isna(row['LON']) == False):\n",
    "        coordsURI = 'lat' + str(row['LAT']) + 'lon' + str(row['LON'])\n",
    "        g.add((CrimeEvent, LAO['occurredInLocation'], LAO[coordsURI]))\n",
    "\n",
    "\n",
    "print(\"--- saving serialization ---\")\n",
    "with open('crimeCrimeEvents.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the club dataframe\n",
    "for index, row in crimeEvents.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the club id as URI\n",
    "    idU = \"crimeEvent\"+str(index)\n",
    "    CrimeEvent = URIRef(LAO[idU])\n",
    "    \n",
    "    # Add triples using store's add() method.\n",
    "    \n",
    "    # TYPE\n",
    "    g.add((CrimeEvent, RDF.type, LAO.CrimeEvent))\n",
    "\n",
    "    # DATA PROPERTIES\n",
    "    g.add((CrimeEvent, LAO['crimeId'], Literal(str(index), datatype=XSD.string)))\n",
    "    part = int(row['Part 1-2'])\n",
    "    if (part == 2):\n",
    "        g.add((CrimeEvent, LAO['reportedToFbi'], Literal(True, datatype=XSD.boolean)))\n",
    "    else:    \n",
    "        g.add((CrimeEvent, LAO['reportedToFbi'], Literal(False, datatype=XSD.boolean)))\n",
    "\n",
    "    # OBJECT PROPERTIES\n",
    "    if(pd.isna(row[\"Date Rptd\"]) == False):\n",
    "        g.add((CrimeEvent, LAO['reportedOnDate'], LAO['day' + str(row['Date Rptd'])]))\n",
    "    if(pd.isna(row[\"DATE OCC\"]) == False):\n",
    "        g.add((CrimeEvent, LAO['occuredOnDate'], LAO['day' + str(row['DATE OCC'])]))\n",
    "    if(pd.isna(row[\"Crm Cd\"]) == False):\n",
    "        g.add((CrimeEvent, LAO['isOfType'], LAO['crime' + str(row['Crm Cd'])]))\n",
    "    if(pd.isna(row[\"Mocodes\"]) == False):\n",
    "        #g.add((CrimeEvent, LAO['occuredOnDate'], LAO['day' + str(row['DATE OCC'])]))\n",
    "        mocodes_list = row['Mocodes'].split()\n",
    "        mocodes_integers = [int(code) for code in mocodes_list]\n",
    "        print(mocodes_integers)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "     \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'clubs.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "playersFifa = pd.read_csv(playersFifaUrl, sep=',', index_col='sofifa_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='Name', keep_default_na=False, na_values=['_'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the players dataframe\n",
    "for index, row in players.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the player id as URI\n",
    "    idU = \"player\"+str(index)\n",
    "    Player = URIRef(SO[idU])\n",
    "    # the transferMarkt profile has as URI, the URL of the profile in the website\n",
    "    TransfermarktProfile = URIRef(row['url'])\n",
    "    \n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Player, RDF.type, SO.Player))\n",
    "    g.add((TransfermarktProfile, RDF.type, SO.TransfermarktProfile))\n",
    "    g.add((TransfermarktProfile, SO['isAbout'], Player))\n",
    "    \n",
    "    #process player name\n",
    "    name = row['name'].split('-')\n",
    "\n",
    "    if (len(name)>1):\n",
    "        g.add((Player, SO['firstName'], Literal(name[0], datatype=XSD.string)))\n",
    "        g.add((Player, SO['lastName'], Literal(name[1], datatype=XSD.string)))\n",
    "    else:\n",
    "        g.add((Player, SO['lastName'], Literal(name[0], datatype=XSD.string)))\n",
    "        \n",
    "    #there can be more than one position per player\n",
    "    for pos in row['position'].split(' - '):\n",
    "        g.add((Player, SO['position'], Literal(pos.lower(), datatype=XSD.string)))\n",
    "    \n",
    "    if not(row['club_id']==''):\n",
    "        idC = \"club\"+str(row['club_id'])\n",
    "        g.add((Player, SO['playFor'], URIRef(SO[idC])))\n",
    "\n",
    "#iterate over the fifa dataframe\n",
    "for index, row in playersFifa.iterrows():\n",
    "    pname = row['short_name'].lower()\n",
    "    if ('.' in pname):\n",
    "        # get last name\n",
    "        # in the fifa dataset we have short names as L. Messi so we delete the L. \n",
    "        # we need to check if the last name contains a space\n",
    "        pname = row['short_name'].split('.')[1].lower().strip()\n",
    "        if ' ' in pname:\n",
    "            i = 0\n",
    "            for t in pname.split(' '):\n",
    "                if i == 0:\n",
    "                    pname = t.lower()\n",
    "                else:\n",
    "                    pname = pname + \"-\" + t.lower()\n",
    "                i += 1           \n",
    "    elif(' ' in pname):\n",
    "        # here we have to handle Cristiano Ronaldo mapping it to cristiano-ronaldo to maximize the match in the players dataframe \n",
    "        i = 0\n",
    "        for t in row['short_name'].split(' '):\n",
    "            if i == 0:\n",
    "                pname = t.lower()\n",
    "            else:\n",
    "                pname = pname + \"-\" + t.lower()\n",
    "            i += 1\n",
    "    pname = strip_accents(pname)\n",
    "    \n",
    "    # find sim with the full name \n",
    "    fullname = row['long_name'].lower()\n",
    "    i = 0\n",
    "    for t in fullname.split(' '):\n",
    "        if i == 0:\n",
    "            fullname = t.lower()\n",
    "        else:\n",
    "            fullname = fullname + \"-\" + t.lower()\n",
    "        i += 1 \n",
    "    fullname = strip_accents(fullname)\n",
    "    # check the players with that last name\n",
    "    names =  players[players['name'].str.contains(pname)]['name']\n",
    "    #find max similarity    \n",
    "    maxN = 0\n",
    "    playerId = ''\n",
    "    for n in names:\n",
    "        sim = SequenceMatcher(None, fullname, n).ratio()\n",
    "        if (maxN < sim):\n",
    "            maxN = sim\n",
    "            playerId = players.loc[players['name'] == n].index[0]\n",
    "        \n",
    "    #if we get a valid playerId we can connect the Fifa stats to the transfermrkt player\n",
    "    if (playerId != ''):\n",
    "        #remove the row from the player dataframe to avoid futher matchings (we know data will contain errors)\n",
    "        players = players.drop(index=playerId)\n",
    "        idU = \"player\"+str(playerId)\n",
    "        Player = URIRef(SO[idU])\n",
    "        g.add((Player, SO['overallFifaValue'], Literal(row['overall'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['growthFifaPotential'], Literal(row['potential'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['economicValue'], Literal(row['value_eur'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['annualWage'], Literal(row['wage_eur'], datatype=XSD.int))) \n",
    "        \n",
    "        pFeatures = str(row['player_tags'])\n",
    "        if pFeatures != '_' and pFeatures != '':\n",
    "            pFeatures = pFeatures.split(',')\n",
    "            for feature in pFeatures:\n",
    "                feature = feature.strip()\n",
    "                feature = re.sub('#', '', feature)\n",
    "                g.add((Player, SO['playerFeature'], Literal(feature, datatype=XSD.string)))\n",
    "        \n",
    "        if row['contract_valid_until'] != '_' and row['contract_valid_until'] != '':\n",
    "            g.add((Player, SO['contractValidTo'], Literal(int(row['contract_valid_until']), datatype=XSD.gYear)))        \n",
    "\n",
    "        g.add((Player, SO['birthday'], Literal(row['dob'], datatype=XSD.date)))\n",
    "        g.add((Player, SO['height'], Literal(row['height_cm'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['weight'], Literal(row['weight_kg'], datatype=XSD.int)))\n",
    "        \n",
    "        \n",
    "        nationality = row['nationality'] \n",
    "        nationality = nationality.replace(\" \", \"_\")\n",
    "        # create the RDF node\n",
    "        Country = URIRef(CNS[nationality])\n",
    "        # add the edge connecting the Movie and the Country \n",
    "        g.add((Player, SO['nationality'], Country))   \n",
    "\n",
    "        # Homework: extend the code to populate the 'propertyOf' edge\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'players.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "apps = pd.read_csv(appearancesUrl, sep=',', index_col='appearance_id', keep_default_na=False, na_values=['_'])\n",
    "games = pd.read_csv(gamesUrl, sep=',', index_col='game_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over the games dataframe\n",
    "for index, row in games.iterrows():\n",
    "    # we use the transfermrket URL as URI\n",
    "    Game = URIRef(row['url'])\n",
    "    g.add((Game, RDF.type, SO.Game))\n",
    "    idU1 = \"club\"+str(row['home_club_id'])\n",
    "    idU2 = \"club\"+str(row['away_club_id'])\n",
    "    HomeClub = URIRef(SO[idU1])\n",
    "    AwayClub = URIRef(SO[idU2])\n",
    "    g.add((Game, SO['homeClub'], HomeClub))\n",
    "    g.add((Game, SO['awayClub'], AwayClub))    \n",
    "    g.add((Game, SO['matchDay'], Literal(row['date'], datatype=XSD.date)))\n",
    "    g.add((Game, SO['homeClubGoals'], Literal(row['home_club_goals'], datatype=XSD.int)))\n",
    "    g.add((Game, SO['awayClubGoals'], Literal(row['away_club_goals'], datatype=XSD.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'games.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the full players dataframe\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldgameid = ''\n",
    "for index, row in apps.iterrows():\n",
    "    idA = \"appearance\"+str(index)\n",
    "    idP = \"player\"+str(row['player_id'])\n",
    "    idG = \"game\"+currgameid\n",
    "    Appearance = URIRef(SO[idA])\n",
    "    Player = URIRef(SO[idP])\n",
    "    currgameid = str(row['game_id'])\n",
    "    Game = URIRef(SO[idG])\n",
    "    g.add((Appearance, RDF.type, SO.Appearance))\n",
    "    g.add((Player, SO['appearIn'], Appearance))\n",
    "    g.add((Appearance, SO['playIn'], Game))\n",
    "\n",
    "    g.add((Appearance, SO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['minutesPlayed'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['yellowCard'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['redCard'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "\n",
    "    #add this triple only once per game\n",
    "    if (currgameid != oldgameid):\n",
    "        idL = \"league\"+str(row['league_id'])\n",
    "        g.add((Game, SO['belongTo'], URIRef(SO[idL])))\n",
    "        oldgameid = currgameid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'appearances.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
