{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the LosAngelesCovid Ontology with the crimes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "# CHECK DATE \n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "activeBusinessesData = '../datasets/ACTIVE BUSINESSES/Listing_of_Active_Businesses_parsed.csv'\n",
    "closedBusinessesData = '../datasets/CLOSED BUSINESSES/All_Closed_Businesses_20231101.csv'\n",
    "laCovidData = '../datasets/COVID DATA/sorted_los_angeles_covid_data.csv'\n",
    "crimeData1 = '../datasets/CRIME DATA/Crime_Data_from_2020_to_Present_1.csv'\n",
    "crimeData2 = '../datasets/CRIME DATA/Crime_Data_from_2020_to_Present_2.csv'\n",
    "crimeData3 = '../datasets/CRIME DATA/Crime_Data_from_2020_to_Present_3.csv'\n",
    "crimeCodesDescData = '../datasets/CRIME DATA/CrimesCodesAndDesc_listed.csv'\n",
    "moCodesData = '../datasets/CRIME DATA/MO_CODES_Numerical_20191119.csv'\n",
    "weaponData = '../datasets/CRIME DATA/weapon_ds.csv'\n",
    "premisCodesData = '../datasets/CRIME DATA/premisCodesData.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "LAO = Namespace(\"http://www.bitsei.it/losAngelesOntology/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime Victims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Vict Age Vict Sex Vict Descent\n",
      "0        36        F            B\n",
      "1        25        M            H\n",
      "2         0        X            X\n",
      "3        76        F            W\n",
      "4        31        X            X\n",
      "   Vict Age Vict Sex Vict Descent\n",
      "0         0      NaN          NaN\n",
      "1        35        F            H\n",
      "2        62        F            B\n",
      "3        33        F            B\n",
      "4        58        M            B\n",
      "   Vict Age Vict Sex Vict Descent\n",
      "0        82        M            B\n",
      "1        38        F            H\n",
      "2        27        F            H\n",
      "3         0      NaN          NaN\n",
      "4        29        M            H\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "laVictims1 = pd.read_csv(crimeData1, sep=',', usecols=['Vict Age', 'Vict Sex', 'Vict Descent']) \n",
    "laVictims2 = pd.read_csv(crimeData2, sep=',', usecols=['Vict Age', 'Vict Sex', 'Vict Descent'])\n",
    "laVictims3 = pd.read_csv(crimeData3, sep=',', usecols=['Vict Age', 'Vict Sex', 'Vict Descent'])\n",
    "\n",
    "print(laVictims1.head())\n",
    "print(laVictims2.head())\n",
    "print(laVictims3.head())\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 55.1 s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "# Create a set to store unique combinations of sex, age, and descent\n",
    "unique_entries = set()\n",
    "\n",
    "#iterate over the laVictims1 dataframe\n",
    "for index, row in laVictims1.iterrows():\n",
    "    \n",
    "    # Extract information from the current row\n",
    "    victim_age = int(row['Vict Age'])\n",
    "    if(victim_age < 0):\n",
    "        victim_age = 0\n",
    "    \n",
    "    victim_sex = str(row['Vict Sex'])\n",
    "    if(victim_sex == 'nan' or victim_sex == '-'):\n",
    "        victim_sex = 'X'\n",
    "\n",
    "    victim_descent = str(row['Vict Descent'])\n",
    "    if(victim_descent == 'nan' or victim_descent == '-'):\n",
    "        victim_descent = 'X'\n",
    "    \n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the victim id as URI\n",
    "    idU = \"victim-\"+str(victim_sex)+'-'+str(victim_age)+'-'+str(victim_descent)\n",
    "    Victim = URIRef(LAO[idU])\n",
    "\n",
    "    # If the current combination doesn't already exist\n",
    "    if ((victim_sex, victim_age, victim_descent) not in unique_entries) and ((victim_age != 0) or (victim_sex != 'X') or (victim_descent != 'X')):\n",
    "        # Add triples using store's add() method.\n",
    "        g.add((Victim, RDF.type, LAO.Victim))\n",
    "        g.add((Victim, LAO['victimSex'], Literal(str(victim_sex), datatype=XSD.string)))\n",
    "        g.add((Victim, LAO['victimAge'], Literal((victim_age), datatype=XSD.int)))    \n",
    "        g.add((Victim, LAO['victimDescent'], Literal(str(victim_descent), datatype=XSD.string)))    \n",
    "\n",
    "        unique_entries.add((victim_sex, victim_age, victim_descent))\n",
    "    \n",
    "    \n",
    "#iterate over the laVictims2 dataframe\n",
    "for index, row in laVictims2.iterrows():\n",
    "    \n",
    "    # Extract information from the current row\n",
    "    victim_age = int(row['Vict Age'])\n",
    "    if(victim_age < 0):\n",
    "        victim_age = 0\n",
    "    \n",
    "    victim_sex = str(row['Vict Sex'])\n",
    "    if(victim_sex == 'nan'):\n",
    "        victim_sex = 'X'\n",
    "\n",
    "    victim_descent = str(row['Vict Descent'])\n",
    "    if(victim_descent == 'nan'):\n",
    "        victim_descent = 'X'\n",
    "    \n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the victim id as URI\n",
    "    idU = \"victim-\"+str(victim_sex)+'-'+str(victim_age)+'-'+str(victim_descent)\n",
    "    Victim = URIRef(LAO[idU])\n",
    "\n",
    "    # If the current combination doesn't already exist\n",
    "    if ((victim_sex, victim_age, victim_descent) not in unique_entries) and ((victim_age != 0) or (victim_sex != 'X') or (victim_descent != 'X')):\n",
    "        # Add triples using store's add() method.\n",
    "        g.add((Victim, RDF.type, LAO.Victim))\n",
    "        g.add((Victim, LAO['victimSex'], Literal(str(victim_sex), datatype=XSD.string)))\n",
    "        g.add((Victim, LAO['victimAge'], Literal((victim_age), datatype=XSD.int)))    \n",
    "        g.add((Victim, LAO['victimDescent'], Literal(str(victim_descent), datatype=XSD.string)))    \n",
    "\n",
    "        unique_entries.add((victim_sex, victim_age, victim_descent))\n",
    "\n",
    "\n",
    "#iterate over the laVictims3 dataframe\n",
    "for index, row in laVictims3.iterrows():\n",
    "    \n",
    "    # Extract information from the current row\n",
    "    victim_age = int(row['Vict Age'])\n",
    "    if(victim_age < 0):\n",
    "        victim_age = 0\n",
    "    \n",
    "    victim_sex = str(row['Vict Sex'])\n",
    "    if(victim_sex == 'nan'):\n",
    "        victim_sex = 'X'\n",
    "\n",
    "    victim_descent = str(row['Vict Descent'])\n",
    "    if(victim_descent == 'nan'):\n",
    "        victim_descent = 'X'\n",
    "    \n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the victim id as URI\n",
    "    idU = \"victim-\"+str(victim_sex)+'-'+str(victim_age)+'-'+str(victim_descent)\n",
    "    Victim = URIRef(LAO[idU])\n",
    "\n",
    "    # If the current combination doesn't already exist\n",
    "    if ((victim_sex, victim_age, victim_descent) not in unique_entries) and ((victim_age != 0) or (victim_sex != 'X') or (victim_descent != 'X')):\n",
    "        # Add triples using store's add() method.\n",
    "        g.add((Victim, RDF.type, LAO.Victim))\n",
    "        g.add((Victim, LAO['victimSex'], Literal(str(victim_sex), datatype=XSD.string)))\n",
    "        g.add((Victim, LAO['victimAge'], Literal((victim_age), datatype=XSD.int)))    \n",
    "        g.add((Victim, LAO['victimDescent'], Literal(str(victim_descent), datatype=XSD.string)))    \n",
    "\n",
    "        unique_entries.add((victim_sex, victim_age, victim_descent))\n",
    "\n",
    "print(\"--- saving serialization ---\")\n",
    "with open('crimeVictims.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modus Operandi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(moCodesData, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "modified_lines = [line.replace(',', '>', 1) for line in lines]\n",
    "\n",
    "with open(moCodesData, 'w') as file:\n",
    "    file.writelines(modified_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 139, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\39392\\OneDrive\\Desktop\\DB2\\bitsei-db2-unipd\\serialization\\serializeLAcrimes.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/39392/OneDrive/Desktop/DB2/bitsei-db2-unipd/serialization/serializeLAcrimes.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the CSV files in memory\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/39392/OneDrive/Desktop/DB2/bitsei-db2-unipd/serialization/serializeLAcrimes.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m modusOperandi \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(moCodesData, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m>\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/39392/OneDrive/Desktop/DB2/bitsei-db2-unipd/serialization/serializeLAcrimes.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(modusOperandi\u001b[39m.\u001b[39mhead())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/39392/OneDrive/Desktop/DB2/bitsei-db2-unipd/serialization/serializeLAcrimes.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#create the graph\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 139, saw 3\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files in memory\n",
    "modusOperandi = pd.read_csv(moCodesData, sep='>')\n",
    "print(modusOperandi.head())\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the laVictims1 dataframe\n",
    "for index, row in modusOperandi.iterrows():\n",
    "    \n",
    "    # Extract information from the current row\n",
    "    print(f\"{row['Code']} - {row['Description']}\")\n",
    "\n",
    "    moCode = int(row['Code'])\n",
    "    moDesc = str(row['Description'])\n",
    "    \n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the modusOperandi Code as URI\n",
    "    idU = \"modusOperandi\"+str(moCode)\n",
    "    Mo = URIRef(LAO[idU])\n",
    "\n",
    "    g.add((Mo, RDF.type, LAO.ModusOperandi))\n",
    "    g.add((Mo, LAO['moCode'], Literal((moCode), datatype=XSD.int)))    \n",
    "    g.add((Mo, LAO['moDesc'], Literal(str(moDesc), datatype=XSD.string)))    \n",
    "    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('crimeModusOperandi.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weapons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(weaponData, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "modified_lines = [line.replace(',', '>', 1) for line in lines]\n",
    "\n",
    "with open(weaponData, 'w') as file:\n",
    "    file.writelines(modified_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "weapons = pd.read_csv(weaponData, sep='>')\n",
    "print(weapons.head())\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the laVictims1 dataframe\n",
    "for index, row in weapons.iterrows():\n",
    "    \n",
    "    # Extract information from the current row\n",
    "    print(f\"{row['Weapon Used Cd']} - {row['Weapon Desc']}\")\n",
    "\n",
    "    weaponCode = int(row['Weapon Used Cd'])\n",
    "    weaponDesc = str(row['Weapon Desc'])\n",
    "    \n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the modusOperandi Code as URI\n",
    "    idU = \"weapon\"+str(weaponCode)\n",
    "    Weapon = URIRef(LAO[idU])\n",
    "\n",
    "    g.add((Weapon, RDF.type, LAO.ModusOperandi))\n",
    "    g.add((Weapon, LAO['weaponCode'], Literal((weaponCode), datatype=XSD.int)))    \n",
    "    g.add((Weapon, LAO['weaponDesc'], Literal(str(weaponDesc), datatype=XSD.string)))    \n",
    "    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('crimeWeapons.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premis Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(premisCodesData, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "modified_lines = [line.replace(',', '>', 1) for line in lines]\n",
    "\n",
    "with open(premisCodesData, 'w') as file:\n",
    "    file.writelines(modified_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "premisCodes = pd.read_csv(premisCodesData, sep='>')\n",
    "print(premisCodes.head())\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"lao\", LAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the laVictims1 dataframe\n",
    "for index, row in premisCodes.iterrows():\n",
    "    \n",
    "    # Extract information from the current row\n",
    "    print(f\"{row['Premis Cd']} - {row['Premis Desc']}\")\n",
    "\n",
    "    premisCode = int(row['Premis Cd'])\n",
    "    premisDesc = str(row['Premis Desc'])\n",
    "    \n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the modusOperandi Code as URI\n",
    "    idU = \"premis\"+str(premisCode)\n",
    "    Premis = URIRef(LAO[idU])\n",
    "\n",
    "    g.add((Premis, RDF.type, LAO.Premis))\n",
    "    g.add((Premis, LAO['premisCode'], Literal((premisCode), datatype=XSD.int)))    \n",
    "    g.add((Premis, LAO['premisDesc'], Literal(str(premisDesc), datatype=XSD.string)))    \n",
    "    \n",
    "print(\"--- saving serialization ---\")\n",
    "with open('crimePremis.ttl', 'w') as file:\n",
    "        file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "clubs = pd.read_csv(clubsUrl, sep=',', index_col='club_id')\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the club dataframe\n",
    "for index, row in clubs.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the club id as URI\n",
    "    idU = \"club\"+str(index)\n",
    "    Club = URIRef(SO[idU])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Club, RDF.type, SO.Club))\n",
    "    g.add((Club, SO['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "    idL = \"league\"+str(row['league_id'])\n",
    "    g.add((Club, SO['competeIn'], URIRef(SO[idL])))\n",
    "    \n",
    "    try:\n",
    "        # get the nationality of the club\n",
    "        nationality = leagues.loc[row['league_id'], 'nationality' ]\n",
    "        # create the RDF node\n",
    "        Country = URIRef(CNS[row['nationality']])\n",
    "        # add the edge connecting the Movie and the Country \n",
    "        g.add((Club, SO['nationality'], Country))    \n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'clubs.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "playersFifa = pd.read_csv(playersFifaUrl, sep=',', index_col='sofifa_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='Name', keep_default_na=False, na_values=['_'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the players dataframe\n",
    "for index, row in players.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the player id as URI\n",
    "    idU = \"player\"+str(index)\n",
    "    Player = URIRef(SO[idU])\n",
    "    # the transferMarkt profile has as URI, the URL of the profile in the website\n",
    "    TransfermarktProfile = URIRef(row['url'])\n",
    "    \n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Player, RDF.type, SO.Player))\n",
    "    g.add((TransfermarktProfile, RDF.type, SO.TransfermarktProfile))\n",
    "    g.add((TransfermarktProfile, SO['isAbout'], Player))\n",
    "    \n",
    "    #process player name\n",
    "    name = row['name'].split('-')\n",
    "\n",
    "    if (len(name)>1):\n",
    "        g.add((Player, SO['firstName'], Literal(name[0], datatype=XSD.string)))\n",
    "        g.add((Player, SO['lastName'], Literal(name[1], datatype=XSD.string)))\n",
    "    else:\n",
    "        g.add((Player, SO['lastName'], Literal(name[0], datatype=XSD.string)))\n",
    "        \n",
    "    #there can be more than one position per player\n",
    "    for pos in row['position'].split(' - '):\n",
    "        g.add((Player, SO['position'], Literal(pos.lower(), datatype=XSD.string)))\n",
    "    \n",
    "    if not(row['club_id']==''):\n",
    "        idC = \"club\"+str(row['club_id'])\n",
    "        g.add((Player, SO['playFor'], URIRef(SO[idC])))\n",
    "\n",
    "#iterate over the fifa dataframe\n",
    "for index, row in playersFifa.iterrows():\n",
    "    pname = row['short_name'].lower()\n",
    "    if ('.' in pname):\n",
    "        # get last name\n",
    "        # in the fifa dataset we have short names as L. Messi so we delete the L. \n",
    "        # we need to check if the last name contains a space\n",
    "        pname = row['short_name'].split('.')[1].lower().strip()\n",
    "        if ' ' in pname:\n",
    "            i = 0\n",
    "            for t in pname.split(' '):\n",
    "                if i == 0:\n",
    "                    pname = t.lower()\n",
    "                else:\n",
    "                    pname = pname + \"-\" + t.lower()\n",
    "                i += 1           \n",
    "    elif(' ' in pname):\n",
    "        # here we have to handle Cristiano Ronaldo mapping it to cristiano-ronaldo to maximize the match in the players dataframe \n",
    "        i = 0\n",
    "        for t in row['short_name'].split(' '):\n",
    "            if i == 0:\n",
    "                pname = t.lower()\n",
    "            else:\n",
    "                pname = pname + \"-\" + t.lower()\n",
    "            i += 1\n",
    "    pname = strip_accents(pname)\n",
    "    \n",
    "    # find sim with the full name \n",
    "    fullname = row['long_name'].lower()\n",
    "    i = 0\n",
    "    for t in fullname.split(' '):\n",
    "        if i == 0:\n",
    "            fullname = t.lower()\n",
    "        else:\n",
    "            fullname = fullname + \"-\" + t.lower()\n",
    "        i += 1 \n",
    "    fullname = strip_accents(fullname)\n",
    "    # check the players with that last name\n",
    "    names =  players[players['name'].str.contains(pname)]['name']\n",
    "    #find max similarity    \n",
    "    maxN = 0\n",
    "    playerId = ''\n",
    "    for n in names:\n",
    "        sim = SequenceMatcher(None, fullname, n).ratio()\n",
    "        if (maxN < sim):\n",
    "            maxN = sim\n",
    "            playerId = players.loc[players['name'] == n].index[0]\n",
    "        \n",
    "    #if we get a valid playerId we can connect the Fifa stats to the transfermrkt player\n",
    "    if (playerId != ''):\n",
    "        #remove the row from the player dataframe to avoid futher matchings (we know data will contain errors)\n",
    "        players = players.drop(index=playerId)\n",
    "        idU = \"player\"+str(playerId)\n",
    "        Player = URIRef(SO[idU])\n",
    "        g.add((Player, SO['overallFifaValue'], Literal(row['overall'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['growthFifaPotential'], Literal(row['potential'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['economicValue'], Literal(row['value_eur'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['annualWage'], Literal(row['wage_eur'], datatype=XSD.int))) \n",
    "        \n",
    "        pFeatures = str(row['player_tags'])\n",
    "        if pFeatures != '_' and pFeatures != '':\n",
    "            pFeatures = pFeatures.split(',')\n",
    "            for feature in pFeatures:\n",
    "                feature = feature.strip()\n",
    "                feature = re.sub('#', '', feature)\n",
    "                g.add((Player, SO['playerFeature'], Literal(feature, datatype=XSD.string)))\n",
    "        \n",
    "        if row['contract_valid_until'] != '_' and row['contract_valid_until'] != '':\n",
    "            g.add((Player, SO['contractValidTo'], Literal(int(row['contract_valid_until']), datatype=XSD.gYear)))        \n",
    "\n",
    "        g.add((Player, SO['birthday'], Literal(row['dob'], datatype=XSD.date)))\n",
    "        g.add((Player, SO['height'], Literal(row['height_cm'], datatype=XSD.int)))\n",
    "        g.add((Player, SO['weight'], Literal(row['weight_kg'], datatype=XSD.int)))\n",
    "        \n",
    "        \n",
    "        nationality = row['nationality'] \n",
    "        nationality = nationality.replace(\" \", \"_\")\n",
    "        # create the RDF node\n",
    "        Country = URIRef(CNS[nationality])\n",
    "        # add the edge connecting the Movie and the Country \n",
    "        g.add((Player, SO['nationality'], Country))   \n",
    "\n",
    "        # Homework: extend the code to populate the 'propertyOf' edge\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'players.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "apps = pd.read_csv(appearancesUrl, sep=',', index_col='appearance_id', keep_default_na=False, na_values=['_'])\n",
    "games = pd.read_csv(gamesUrl, sep=',', index_col='game_id', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over the games dataframe\n",
    "for index, row in games.iterrows():\n",
    "    # we use the transfermrket URL as URI\n",
    "    Game = URIRef(row['url'])\n",
    "    g.add((Game, RDF.type, SO.Game))\n",
    "    idU1 = \"club\"+str(row['home_club_id'])\n",
    "    idU2 = \"club\"+str(row['away_club_id'])\n",
    "    HomeClub = URIRef(SO[idU1])\n",
    "    AwayClub = URIRef(SO[idU2])\n",
    "    g.add((Game, SO['homeClub'], HomeClub))\n",
    "    g.add((Game, SO['awayClub'], AwayClub))    \n",
    "    g.add((Game, SO['matchDay'], Literal(row['date'], datatype=XSD.date)))\n",
    "    g.add((Game, SO['homeClubGoals'], Literal(row['home_club_goals'], datatype=XSD.int)))\n",
    "    g.add((Game, SO['awayClubGoals'], Literal(row['away_club_goals'], datatype=XSD.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'games.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the full players dataframe\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id', keep_default_na=False, na_values=['_'])\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldgameid = ''\n",
    "for index, row in apps.iterrows():\n",
    "    idA = \"appearance\"+str(index)\n",
    "    idP = \"player\"+str(row['player_id'])\n",
    "    idG = \"game\"+currgameid\n",
    "    Appearance = URIRef(SO[idA])\n",
    "    Player = URIRef(SO[idP])\n",
    "    currgameid = str(row['game_id'])\n",
    "    Game = URIRef(SO[idG])\n",
    "    g.add((Appearance, RDF.type, SO.Appearance))\n",
    "    g.add((Player, SO['appearIn'], Appearance))\n",
    "    g.add((Appearance, SO['playIn'], Game))\n",
    "\n",
    "    g.add((Appearance, SO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['minutesPlayed'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['yellowCard'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "    g.add((Appearance, SO['redCard'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "\n",
    "    #add this triple only once per game\n",
    "    if (currgameid != oldgameid):\n",
    "        idL = \"league\"+str(row['league_id'])\n",
    "        g.add((Game, SO['belongTo'], URIRef(SO[idL])))\n",
    "        oldgameid = currgameid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'appearances.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle').decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
